#!/usr/bin/env python3
"""
Corpus Analysis Orchestrator

Runs complete corpus analysis pipeline to answer:
1. Should DR framework extend with new categories (Piton, Scaffold, Tangled Rope, Wings)?
2. Do existing 4 indices explain all variance?

Generates three reports:
- variance_analysis.md - Index variance analysis
- index_sufficiency.md - Index sufficiency test
- pattern_mining.md - Structural pattern mining
"""

import sys
import subprocess
from pathlib import Path
from datetime import datetime

class CorpusAnalysisOrchestrator:
    """Orchestrates the complete corpus analysis pipeline"""

    def __init__(self, output_txt='../outputs/output.txt', testsets_dir='../prolog/testsets/', output_dir='../outputs/'):
        self.output_txt = Path(output_txt)
        self.testsets_dir = Path(testsets_dir)
        self.output_dir = Path(output_dir)

        # Intermediate data file
        self.corpus_data = self.output_dir / 'corpus_data.json'

        # Report files
        self.variance_report = self.output_dir / 'variance_analysis.md'
        self.sufficiency_report = self.output_dir / 'index_sufficiency.md'
        self.pattern_report = self.output_dir / 'pattern_mining.md'

    def run_pipeline(self):
        """Execute complete analysis pipeline"""
        print("="*80)
        print("CORPUS ANALYSIS PIPELINE".center(80))
        print("="*80)
        print(f"\nStarted: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        # Step 1: Extract corpus data
        print("Step 1: Extracting corpus data...")
        print("-" * 80)
        if not self._run_extraction():
            print("\nâŒ Extraction failed. Aborting pipeline.")
            return False

        print("âœ“ Corpus data extracted\n")

        # Step 2: Generate variance analysis
        print("Step 2: Generating variance analysis report...")
        print("-" * 80)
        if not self._run_variance_analysis():
            print("\nâŒ Variance analysis failed. Aborting pipeline.")
            return False

        print("âœ“ Variance analysis complete\n")

        # Step 3: Generate sufficiency test
        print("Step 3: Running index sufficiency test...")
        print("-" * 80)
        if not self._run_sufficiency_test():
            print("\nâŒ Sufficiency test failed. Aborting pipeline.")
            return False

        print("âœ“ Sufficiency test complete\n")

        # Step 4: Generate pattern mining
        print("Step 4: Mining structural patterns...")
        print("-" * 80)
        if not self._run_pattern_mining():
            print("\nâŒ Pattern mining failed. Aborting pipeline.")
            return False

        print("âœ“ Pattern mining complete\n")

        # Final summary
        self._print_summary()

        return True

    def _run_extraction(self):
        """Run data extraction"""
        try:
            result = subprocess.run([
                'python3', 'extract_corpus_data.py',
                '--output-txt', str(self.output_txt),
                '--testsets', str(self.testsets_dir),
                '--json-output', str(self.corpus_data)
            ], capture_output=True, text=True, check=True)

            print(result.stdout)
            return True

        except subprocess.CalledProcessError as e:
            print(f"Error: {e.stderr}")
            return False
        except Exception as e:
            print(f"Unexpected error: {e}")
            return False

    def _run_variance_analysis(self):
        """Run variance analysis report"""
        try:
            result = subprocess.run([
                'python3', 'variance_analyzer.py',
                '--corpus-data', str(self.corpus_data),
                '--output', str(self.variance_report)
            ], capture_output=True, text=True, check=True)

            print(result.stdout)
            return True

        except subprocess.CalledProcessError as e:
            print(f"Error: {e.stderr}")
            return False
        except Exception as e:
            print(f"Unexpected error: {e}")
            return False

    def _run_sufficiency_test(self):
        """Run index sufficiency test"""
        try:
            result = subprocess.run([
                'python3', 'sufficiency_tester.py',
                '--corpus-data', str(self.corpus_data),
                '--output', str(self.sufficiency_report)
            ], capture_output=True, text=True, check=True)

            print(result.stdout)
            return True

        except subprocess.CalledProcessError as e:
            print(f"Error: {e.stderr}")
            return False
        except Exception as e:
            print(f"Unexpected error: {e}")
            return False

    def _run_pattern_mining(self):
        """Run pattern mining analysis"""
        try:
            result = subprocess.run([
                'python3', 'pattern_miner.py',
                '--corpus-data', str(self.corpus_data),
                '--output', str(self.pattern_report)
            ], capture_output=True, text=True, check=True)

            print(result.stdout)
            return True

        except subprocess.CalledProcessError as e:
            print(f"Error: {e.stderr}")
            return False
        except Exception as e:
            print(f"Unexpected error: {e}")
            return False

    def _print_summary(self):
        """Print final summary"""
        print("="*80)
        print("PIPELINE COMPLETE".center(80))
        print("="*80)
        print(f"\nCompleted: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        print("ðŸ“Š Generated Reports:\n")
        print(f"  1. Variance Analysis:     {self.variance_report}")
        print(f"  2. Index Sufficiency:     {self.sufficiency_report}")
        print(f"  3. Pattern Mining:        {self.pattern_report}")

        print("\nðŸ“ Intermediate Data:\n")
        print(f"  - Corpus Data (JSON):     {self.corpus_data}")

        print("\n" + "="*80)
        print("NEXT STEPS".center(80))
        print("="*80)
        print("""
1. Review the three reports in order:
   - variance_analysis.md - Understand how constraints vary across indices
   - index_sufficiency.md - Check if 4 indices are sufficient
   - pattern_mining.md - Identify candidate new categories

2. Answer the core questions:
   - Should we add new categories (Piton, Scaffold, Tangled Rope, Wings)?
   - Are the existing 4 indices sufficient?

3. Key indicators to look for:
   - High collision rate (>15%) â†’ Indices may be insufficient
   - Many structural twins â†’ Suggest new categories
   - Large hybrid/transition groups â†’ Consider Tangled Rope, Scaffold
   - Many false mountains â†’ Consider Piton category

4. Make informed decisions:
   - If indices are sufficient â†’ Focus on refining thresholds
   - If indices are insufficient â†’ Add 5th index or new categories
   - If patterns emerge â†’ Formalize new categories

5. Update the DR framework accordingly and re-run tests
""")

        print("="*80 + "\n")

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Run complete corpus analysis pipeline'
    )
    parser.add_argument(
        '--output-txt',
        default='../outputs/output.txt',
        help='Path to output.txt from Prolog tests'
    )
    parser.add_argument(
        '--testsets',
        default='../prolog/testsets/',
        help='Path to testsets directory'
    )
    parser.add_argument(
        '--output-dir',
        default='../outputs/',
        help='Output directory for reports'
    )

    args = parser.parse_args()

    orchestrator = CorpusAnalysisOrchestrator(
        output_txt=args.output_txt,
        testsets_dir=args.testsets,
        output_dir=args.output_dir
    )

    success = orchestrator.run_pipeline()

    sys.exit(0 if success else 1)

if __name__ == '__main__':
    main()
import subprocess
import os
import re
import json

# --- COMPONENT 1: NARRATIVE CHUNKING HANDLER ---
class NarrativeContextHandler:
    def __init__(self, raw_text):
        self.raw_text = raw_text
        self.intervals = []

    def chunk_by_narrative(self, anchor_pattern=r"(Chapter \d+|Section \d+|ARTICLE [IVX]+)"):
        """Chunks document by structural headers to preserve ontological coherence."""
        splits = [m.start() for m in re.finditer(anchor_pattern, self.raw_text)]
        if not splits:
            splits = [0, len(self.raw_text) // 2]
        splits.append(len(self.raw_text))

        for i in range(len(splits) - 1):
            chunk = self.raw_text[splits[i]:splits[i+1]]
            interval_id = f"audit_interval_{i}"
            # Maps to Prolog integer requirements for Start and End
            self.intervals.append({
                "id": interval_id,
                "text": chunk.strip(),
                "t_range": (i * 10, (i + 1) * 10)
            })
        return self.intervals

# --- COMPONENT 2: THE BATCH RUNNER ---
class DRBatchRunner:
    def __init__(self, orchestrator):
        self.orchestrator = orchestrator
        self.unified_audit_log = []
        self.master_prolog_db = "unified_audit_database.pl"

    def run_full_document_audit(self, document_text):
        print("[*] Initiating Narrative-Driven Batch Audit...")

        # 1. Chunking
        handler = NarrativeContextHandler(document_text)
        narrative_intervals = handler.chunk_by_narrative()

        with open(self.master_prolog_db, "w") as master_file:
            master_file.write(":- style_check(-discontiguous).\n") # Prevent Prolog warnings

            for interval in narrative_intervals:
                print(f"\n>>> PROCESSING: {interval['id']} (T:{interval['t_range'][0]}-{interval['t_range'][1]})")

                # 2. Run the Audit Cycle (Omega Scan -> Generation -> Validation)
                # This leverages dr_mismatch/3 and verify_all/0
                success, report_output, prolog_data = self.orchestrator.run_audit_cycle(
                    interval['text'],
                    interval['id']
                )

                # 3. Aggregate results
                self.unified_audit_log.append({
                    "interval": interval['id'],
                    "success": success,
                    "summary": report_output
                })

                # Save facts to unified DB
                master_file.write(f"\n% --- DATA FOR {interval['id']} ---\n")
                master_file.write(prolog_data + "\n")

        self.generate_final_report()

    def generate_final_report(self):
        print("\n" + "="*50)
        print("   UNIFIED DOCUMENT AUDIT COMPLETE")
        print("="*50)
        for entry in self.unified_audit_log:
            status = "[OK]" if entry['success'] else "[FAILED]"
            print(f"{status} {entry['interval']}")

        # Save JSON log for further analysis (optional)
        with open("audit_results.json", "w") as f:
            json.dump(self.unified_audit_log, f, indent=4)
        print(f"\n[*] Unified facts saved to: {self.master_prolog_db}")
import json
import re
import os
from collections import defaultdict

# Path Configuration
REGISTRY_FILE = "../prolog/domain_registry.pl"
ANALYSIS_FILE = "../outputs/structured_analysis.json"

def calibrate_profiles():
    # 1. Load Category Mapping from Registry
    cat_map = {}
    if os.path.exists(REGISTRY_FILE):
        with open(REGISTRY_FILE, 'r') as f:
            content = f.read()
            # Maps both Interval IDs and Constraint IDs to categories
            matches = re.findall(r"domain_category\(([^,]+),\s*([^)]+)\)\.", content)
            for d_id, cat in matches:
                cat_map[d_id.strip()] = cat.strip()
    else:
        print(f"Error: {REGISTRY_FILE} not found.")
        return

    # 2. Load Structured Analysis
    if not os.path.exists(ANALYSIS_FILE):
        print(f"Error: {ANALYSIS_FILE} not found.")
        return

    with open(ANALYSIS_FILE, 'r') as f:
        analysis = json.load(f)

    # 3. Aggregate Imputed Vector Values
    # Profiles: [Accessibility, Stakes, Suppression, Resistance]
    agg = defaultdict(lambda: [[] for _ in range(4)])
    metric_pos = {
        "accessibility_collapse": 0,
        "stakes_inflation": 1,
        "suppression": 2,
        "resistance": 3
    }

    # Tracking domains that contributed to specific categories
    contribution_counts = defaultdict(int)

    for domain_label, record in analysis.items():
        # Use the ID from the path if label mapping is inconsistent
        # e.g., ../prolog/testsets/adverse_possession.pl -> adverse_possession
        path_id = os.path.basename(record["path"]).replace(".pl", "")
        category = cat_map.get(path_id, "unknown_novel")

        if record["repaired_vectors"]:
            contribution_counts[category] += 1
            for rv in record["repaired_vectors"]:
                # Normalize metric name: suppression(class) -> suppression
                base_metric = rv["metric"].split('(')[0]
                if base_metric in metric_pos:
                    pos = metric_pos[base_metric]
                    agg[category][pos].append(float(rv["val"]))

    # 4. Generate Calibrated Profiles
    print("\n% ============================================================================")
    print("% CALIBRATED CATEGORY PROFILES")
    print("% Based on means of imputed vectors from 400+ validated instances.")
    print("% ============================================================================\n")

    for cat, pos_values in sorted(agg.items()):
        means = []
        for values in pos_values:
            if values:
                means.append(round(sum(values) / len(values), 2))
            else:
                means.append(0.5) # Default to neutral if no repairs were needed

        print(f"category_profile({cat:20}, {means}).")

    print(f"\n% Stats: Data derived from {sum(contribution_counts.values())} repaired domains.")

if __name__ == "__main__":
    calibrate_profiles()
import json

def category_mismatch_audit(json_path, registry_path):
    # Load analysis and category registry
    with open(json_path, 'r') as f: analysis = json.load(f)

    # Simple check for Narrative History vs Shadow Noose
    print(f"{'DOMAIN':<40} | {'REPORTED CAT' :<20} | {'ACTUAL SIGNATURE'}")
    print("-" * 80)

    for domain, record in analysis.items():
        repairs = record.get("repaired_vectors", [])
        sup = max([float(r['val']) for r in repairs if 'suppression' in r['metric']] + [0.0])

        # If the domain is a Shadow Noose but classified as History
        if sup >= 0.8:
            print(f"{domain:<40} | {'EXTRACTIVE?':<20} | [CRITICAL] Hidden Extraction Detected")

if __name__ == "__main__":
    category_mismatch_audit("../outputs/structured_analysis.json", "../prolog/domain_registry.pl")
#!/usr/bin/env python3
"""
Corpus Analyzer - Analyzes the constraint corpus for conceptual connections

Identifies:
- Concept clusters and themes
- Missing connections
- Suggested scenarios based on existing patterns
- Knowledge gaps in the corpus
"""

import re
from collections import defaultdict, Counter
from pathlib import Path
import itertools

class CorpusAnalyzer:
    def __init__(self, testsets_dir='../prolog/testsets/'):
        self.testsets_dir = Path(testsets_dir)
        self.constraints = {}  # name -> metadata
        self.domains = defaultdict(list)  # domain -> [constraints]
        self.types = defaultdict(list)  # type -> [constraints]
        self.concepts = defaultdict(set)  # concept -> {constraints mentioning it}
        self.co_occurrences = defaultdict(int)  # (concept1, concept2) -> count

    def extract_concepts_from_name(self, name):
        """Extract concepts from constraint name"""
        # Split on underscores and common separators
        parts = re.split(r'[_\-\s]+', name.lower())

        # Filter out common words and numbers
        stopwords = {'the', 'of', 'and', 'or', 'in', 'on', 'at', 'to', 'for', 'a', 'an',
                    'is', 'are', 'was', 'were', 'be', 'been', 'being',
                    'system', 'interval', 'era', 'cycle'}

        concepts = []
        for part in parts:
            # Skip if it's a number, year, or stopword
            if part.isdigit() or len(part) < 3 or part in stopwords:
                continue
            # Skip if it looks like a year
            if re.match(r'\d{4}', part):
                continue
            concepts.append(part)

        return concepts

    def analyze_corpus(self):
        """Analyze all test files"""
        if not self.testsets_dir.exists():
            print(f"Error: {self.testsets_dir} does not exist")
            return False

        pl_files = list(self.testsets_dir.glob('*.pl'))

        if not pl_files:
            print(f"No .pl files found in {self.testsets_dir}")
            return False

        print(f"Analyzing {len(pl_files)} constraint files...")

        for filepath in pl_files:
            self._analyze_file(filepath)

        # Build concept co-occurrence matrix
        for constraint_name, metadata in self.constraints.items():
            concepts = metadata.get('concepts', [])

            # Record all pairs of co-occurring concepts
            for c1, c2 in itertools.combinations(sorted(concepts), 2):
                self.co_occurrences[(c1, c2)] += 1

        return True

    def _analyze_file(self, filepath):
        """Analyze a single test file"""
        constraint_name = filepath.stem

        metadata = {
            'file': filepath.name,
            'domain': None,
            'type': None,
            'concepts': self.extract_concepts_from_name(constraint_name)
        }

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()

            # Extract domain category
            domain_match = re.search(r"domain_priors:category_of\([^,]+,\s*(\w+)\)", content)
            if domain_match:
                metadata['domain'] = domain_match.group(1)
                self.domains[metadata['domain']].append(constraint_name)

            # Extract claimed type
            type_match = re.search(r"constraint_claim\([^,]+,\s*(\w+)\)", content)
            if type_match:
                metadata['type'] = type_match.group(1)
                self.types[metadata['type']].append(constraint_name)

        except Exception as e:
            print(f"Warning: Could not analyze {filepath.name}: {e}")

        self.constraints[constraint_name] = metadata

        # Index concepts
        for concept in metadata['concepts']:
            self.concepts[concept].add(constraint_name)

    def generate_report(self):
        """Generate corpus analysis report"""
        print("\n" + "="*80)
        print("CORPUS ANALYSIS: Conceptual Connections".center(80))
        print("="*80 + "\n")

        self._section_concept_clusters()
        self._section_domain_gaps()
        self._section_suggested_scenarios()
        self._section_concept_network()

        print("="*80 + "\n")

    def _section_concept_clusters(self):
        """Identify and report concept clusters"""
        print("ðŸ” CONCEPT CLUSTERS")
        print("-" * 80)

        # Find concepts that appear in multiple constraints
        frequent_concepts = {concept: constraints
                           for concept, constraints in self.concepts.items()
                           if len(constraints) >= 3}

        if frequent_concepts:
            print(f"  Found {len(frequent_concepts)} concepts appearing in 3+ constraints:\n")

            for concept, constraints in sorted(frequent_concepts.items(),
                                              key=lambda x: len(x[1]),
                                              reverse=True)[:10]:
                print(f"  {concept:20s}: {len(constraints)} constraint(s)")
                # Show a few examples
                for c in list(constraints)[:3]:
                    print(f"    - {c}")
                if len(constraints) > 3:
                    print(f"    ... and {len(constraints) - 3} more")
                print()
        else:
            print("  No strong concept clusters found (concepts appear in < 3 constraints)")

        print()

    def _section_domain_gaps(self):
        """Identify gaps in domain coverage"""
        print("ðŸŽ¯ DOMAIN GAPS & BALANCE")
        print("-" * 80)

        if not self.domains:
            print("  No domain information available")
            return

        # Show distribution
        print(f"  Domain Distribution ({len(self.domains)} domains):\n")

        for domain, constraints in sorted(self.domains.items(),
                                         key=lambda x: len(x[1]),
                                         reverse=True):
            count = len(constraints)
            bar = 'â–ˆ' * min(count, 50)
            print(f"  {domain:30s}: {count:3d} {bar}")

        # Identify gaps
        print(f"\n  Balance Analysis:")

        avg_count = sum(len(c) for c in self.domains.values()) / len(self.domains)

        overrepresented = [(d, len(c)) for d, c in self.domains.items()
                          if len(c) > avg_count * 1.5]
        underrepresented = [(d, len(c)) for d, c in self.domains.items()
                           if len(c) < 3]

        if overrepresented:
            print(f"\n  âœ“ Well-represented domains:")
            for domain, count in overrepresented[:5]:
                print(f"    - {domain} ({count})")

        if underrepresented:
            print(f"\n  âš  Underrepresented domains (add more scenarios):")
            for domain, count in underrepresented:
                print(f"    - {domain} ({count} constraint(s))")

        print()

    def _section_suggested_scenarios(self):
        """Suggest new scenarios based on patterns"""
        print("ðŸ’¡ SUGGESTED SCENARIOS")
        print("-" * 80)

        suggestions = []

        # 1. Combine concepts that co-occur frequently
        if self.co_occurrences:
            top_pairs = sorted(self.co_occurrences.items(),
                             key=lambda x: x[1],
                             reverse=True)[:5]

            print("  Based on concept co-occurrence patterns:\n")
            for (c1, c2), count in top_pairs:
                # Suggest exploring these together in new context
                suggestion = f"Explore {c1} + {c2} in a new context ({count} existing linkages)"
                print(f"  â€¢ {suggestion}")

        # 2. Cross-pollinate domains
        if len(self.domains) >= 2:
            print("\n  Cross-domain scenario ideas:\n")

            # Find domains with similar concepts
            domain_concepts = {}
            for domain, constraints in self.domains.items():
                concepts = set()
                for c in constraints:
                    if c in self.constraints:
                        concepts.update(self.constraints[c]['concepts'])
                domain_concepts[domain] = concepts

            # Find domains with overlapping concepts
            for d1, d2 in itertools.combinations(list(self.domains.keys())[:10], 2):
                if d1 in domain_concepts and d2 in domain_concepts:
                    overlap = domain_concepts[d1] & domain_concepts[d2]
                    if len(overlap) >= 2:
                        concepts_str = ', '.join(list(overlap)[:3])
                        print(f"  â€¢ {d1} â†” {d2}: shared concepts ({concepts_str})")

        # 3. Fill type gaps
        if self.types:
            print("\n  Type distribution balance:\n")

            for ctype, constraints in self.types.items():
                count = len(constraints)
                print(f"  {ctype:15s}: {count:3d}")

            # Suggest adding underrepresented types
            min_count = min(len(c) for c in self.types.values())
            underrep_types = [t for t, c in self.types.items() if len(c) == min_count]

            if underrep_types and len(underrep_types) < len(self.types):
                print(f"\n  âš  Add more '{', '.join(underrep_types)}' scenarios for balance")

        # 4. Concept-based suggestions
        if self.concepts:
            # Find concepts that appear alone (could be connected to others)
            lonely_concepts = {concept: constraints
                             for concept, constraints in self.concepts.items()
                             if len(constraints) == 1}

            if lonely_concepts and len(lonely_concepts) < 20:
                print(f"\n  Isolated concepts (could be connected to others):\n")
                for concept, constraints in list(lonely_concepts.items())[:5]:
                    constraint = list(constraints)[0]
                    print(f"  â€¢ Expand on '{concept}' (currently only in {constraint})")

        print()

    def _section_concept_network(self):
        """Show strongest conceptual connections"""
        print("ðŸ•¸ï¸  CONCEPT NETWORK")
        print("-" * 80)

        if not self.co_occurrences:
            print("  No concept co-occurrences found")
            return

        # Find strongest connections
        top_connections = sorted(self.co_occurrences.items(),
                                key=lambda x: x[1],
                                reverse=True)[:15]

        print(f"  Strongest conceptual connections ({len(self.co_occurrences)} total):\n")

        for (c1, c2), count in top_connections:
            # Show as a network edge
            strength = 'â”' * min(count, 10)
            print(f"  {c1:20s} {strength}â†’ {c2:20s} ({count})")

        print(f"\n  ðŸ’¡ These connections suggest natural clusters for future scenarios")
        print()

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Analyze corpus for conceptual connections and gaps'
    )
    parser.add_argument(
        '--testsets',
        default='../prolog/testsets/',
        help='Path to testsets directory (default: ../prolog/testsets/)'
    )

    args = parser.parse_args()

    analyzer = CorpusAnalyzer(args.testsets)

    if not analyzer.analyze_corpus():
        sys.exit(1)

    analyzer.generate_report()

if __name__ == '__main__':
    main()
import json
import math
import os

# --- CONFIGURATION ---
# Define the keys used in your JSON structure
DATA_FILE = "../outputs/structured_analysis.json"
OUTPUT_FILE = "../outputs/signature_matches.txt"

def euclidean_distance(v1, v2):
    """Calculates the straight-line distance between two vectors."""
    return math.sqrt(sum((a - b) ** 2 for a, b in zip(v1, v2)))

def analyze_signatures():
    if not os.path.exists(DATA_FILE):
        print(f"Error: {DATA_FILE} not found.")
        return

    with open(DATA_FILE, 'r') as f:
        data = json.load(f)

    # 1. Target Vector: Calibrated 'Extractive Market' profile [Acc, Stk, Sup, Res]
    target_vector = [0.4, 0.8, 0.8, 0.6]

    results = []

    for domain_id, record in data.items():
        vectors = record.get("repaired_vectors", [])
        if not vectors:
            continue

        try:
            # 2. Extract and CAST to float. We target individual-level metrics at t=10.
            # Default to 0.5 (neutral) if a specific metric is missing.
            current_vector = [
                float(next((v['val'] for v in vectors if v['metric'] == "accessibility_collapse(individual)" and v['t'] == "10"), 0.5)),
                float(next((v['val'] for v in vectors if v['metric'] == "stakes_inflation(individual)" and v['t'] == "10"), 0.5)),
                float(next((v['val'] for v in vectors if v['metric'] == "suppression(individual)" and v['t'] == "10"), 0.5)),
                float(next((v['val'] for v in vectors if v['metric'] == "resistance(individual)" and v['t'] == "10"), 0.5))
            ]

            distance = euclidean_distance(target_vector, current_vector)
            results.append({
                "domain": domain_id,
                "distance": distance,
                "vector": current_vector
            })
        except (StopIteration, ValueError, TypeError):
            # Skip records with malformed or missing data
            continue

    # 3. Sort by proximity (lowest distance = highest resemblance)
    results.sort(key=lambda x: x['distance'])

    # 4. Console Output
    print(f"\n{'DOMAIN':<40} | {'DISTANCE':<10} | {'MATCH SCORE'}")
    print("-" * 70)
    for res in results[:15]:
        match_percentage = max(0, 100 * (1 - res['distance']))
        print(f"{res['domain']:<40} | {res['distance']:<10.4f} | {match_percentage:>6.2f}%")

    # 5. File Output
    with open(OUTPUT_FILE, 'w') as out:
        out.write(f"CROSS-DOMAIN SIGNATURE ANALYSIS\nTarget Profile: {target_vector}\n\n")
        for res in results:
            out.write(f"{res['domain']}: Distance {res['distance']:.4f}\n")

if __name__ == "__main__":
    analyze_signatures()
import json
import os

ANALYSIS_FILE = "../outputs/structured_analysis.json"
GAP_FILE = "../outputs/gap_report.json"

def cross_reference():
    with open(ANALYSIS_FILE, 'r') as f: analysis = json.load(f)
    with open(GAP_FILE, 'r') as f: gaps = json.load(f)

    print(f"{'DOMAIN':<40} | {'GAPS':<5} | {'REPAIRS':<8} | {'REPAIR INTENSITY'}")
    print("-" * 75)

    for domain, gap_data in gaps.items():
        if not gap_data["is_complete"]:
            # Find repair count in analysis
            repair_count = len(analysis.get(domain, {}).get("repaired_vectors", []))
            intensity = "HIGH" if repair_count > 4 else "LOW"

            missing = []
            if not gap_data["rope"]: missing.append("ROPE")
            if not gap_data["noose"]: missing.append("NOOSE")
            if not gap_data["mountain"]: missing.append("MTN")

            print(f"{domain:<40} | {len(missing):<5} | {repair_count:<8} | {intensity} (Missing: {', '.join(missing)})")

if __name__ == "__main__":
    cross_reference()
import os
import re
import glob
import argparse

def generate_domain_registry(testsets_dir, output_path):
    # Patterns to capture BOTH the constraint ID and the Interval ID
    constraint_pattern = re.compile(r"constraint_id:\s*([a-zA-Z0-9_]+)")
    interval_pattern = re.compile(r"narrative_ontology:interval\(([a-zA-Z0-9_]+),")
    extractiveness_pattern = re.compile(r"(?:domain_priors:)?base_extractiveness\(([^,]+),\s*([\d\.]+)\)")

    registry = {}

    pl_files = glob.glob(os.path.join(testsets_dir, "*.pl"))

    for file_path in pl_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

                # Find IDs to register
                c_match = constraint_pattern.search(content)
                i_match = interval_pattern.search(content)
                e_match = extractiveness_pattern.search(content)

                ids_to_register = []
                if c_match: ids_to_register.append(c_match.group(1).strip())
                if i_match: ids_to_register.append(i_match.group(1).strip())

                if e_match and ids_to_register:
                    score = float(e_match.group(2))
                    category = "extractive_market" if score > 0.6 else "narrative_history"

                    # Register both IDs to the same category to satisfy the auditor
                    for identifier in ids_to_register:
                        registry[identifier] = category

        except Exception as e:
            print(f"Error processing {file_path}: {e}")

    # Generate Prolog Module
    header = [
        ":- module(domain_registry, [domain_category/2]).",
        "% --- AUTOMATICALLY GENERATED DOMAIN REGISTRY ---",
        "% Maps both Constraint IDs and Interval IDs to categories.",
        ""
    ]

    lines = [f"domain_category({k}, {v})." for k, v in sorted(registry.items())]

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(header + lines))

    print(f"âœ“ Registry generated at {output_path} with {len(registry)} entries.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", default="../prolog/testsets/", help="Path to .pl testsets")
    parser.add_argument("--output", default="../prolog/domain_registry.pl", help="Path to registry output")
    args = parser.parse_args()

    generate_domain_registry(args.input, args.output)
#!/usr/bin/env python3
"""
Domain Priors Expander - v1.0

Analyzes all testset files in prolog/testsets/*.pl to compute corpus averages
and expands domain_priors.pl with reasonable defaults for inputted domains.

Usage:
    python domain_priors_expander.py

Output:
    - Prints corpus statistics to console
    - Generates prolog/domain_priors_expanded.pl with new category profiles
"""

import os
import re
import json
from collections import defaultdict
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple
from pathlib import Path

# Configuration
TESTSETS_DIR = Path(__file__).parent.parent / 'prolog' / 'testsets'
DOMAIN_REGISTRY = Path(__file__).parent.parent / 'prolog' / 'domain_registry.pl'
DOMAIN_PRIORS = Path(__file__).parent.parent / 'prolog' / 'domain_priors.pl'
OUTPUT_FILE = Path(__file__).parent.parent / 'prolog' / 'domain_priors_expanded.pl'
STATS_OUTPUT = Path(__file__).parent.parent / 'outputs' / 'domain_corpus_stats.json'

# Regex patterns for extracting Prolog facts
EXTRACTIVENESS_REGEX = re.compile(
    r"(?:domain_priors:)?base_extractiveness\s*\(\s*['\"]?([a-zA-Z0-9_]+)['\"]?\s*,\s*([0-9.]+)\s*\)"
)
SUPPRESSION_REGEX = re.compile(
    r"(?:domain_priors:)?suppression_score\s*\(\s*['\"]?([a-zA-Z0-9_]+)['\"]?\s*,\s*([0-9.]+)\s*\)"
)
CONSTRAINT_CLAIM_REGEX = re.compile(
    r"(?:narrative_ontology:)?constraint_claim\s*\(\s*['\"]?([a-zA-Z0-9_]+)['\"]?\s*,\s*([a-zA-Z_]+)\s*\)"
)
REQUIRES_ENFORCEMENT_REGEX = re.compile(
    r"(?:domain_priors:)?requires_active_enforcement\s*\(\s*['\"]?([a-zA-Z0-9_]+)['\"]?\s*\)"
)
EMERGES_NATURALLY_REGEX = re.compile(
    r"(?:domain_priors:)?emerges_naturally\s*\(\s*['\"]?([a-zA-Z0-9_]+)['\"]?\s*\)"
)
DOMAIN_CATEGORY_REGEX = re.compile(
    r"domain_category\s*\(\s*['\"]?([a-zA-Z0-9_]+)['\"]?\s*,\s*([a-zA-Z_]+)\s*\)"
)
INTERVAL_REGEX = re.compile(
    r"(?:narrative_ontology:)?interval\s*\(\s*['\"]?([a-zA-Z0-9_]+)['\"]?\s*,\s*(-?[0-9]+)\s*,\s*(-?[0-9]+)\s*\)"
)
CONSTRAINT_METRIC_REGEX = re.compile(
    r"constraint_metric\s*\(\s*['\"]?([a-zA-Z0-9_]+)['\"]?\s*,\s*([a-zA-Z_]+)\s*,\s*([0-9.]+)\s*\)"
)


@dataclass
class ConstraintData:
    """Data extracted from a single testset file."""
    constraint_id: str
    file_path: str
    base_extractiveness: Optional[float] = None
    suppression_score: Optional[float] = None
    constraint_type: Optional[str] = None  # mountain, rope, noose, tangled_rope
    requires_enforcement: bool = False
    emerges_naturally: bool = False
    category: Optional[str] = None
    interval_start: Optional[int] = None
    interval_end: Optional[int] = None
    metrics: Dict[str, float] = field(default_factory=dict)


@dataclass
class CategoryStats:
    """Aggregated statistics for a category."""
    category: str
    count: int = 0
    extractiveness_values: List[float] = field(default_factory=list)
    suppression_values: List[float] = field(default_factory=list)
    constraint_types: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
    enforcement_count: int = 0
    natural_count: int = 0

    @property
    def avg_extractiveness(self) -> float:
        return sum(self.extractiveness_values) / len(self.extractiveness_values) if self.extractiveness_values else 0.5

    @property
    def avg_suppression(self) -> float:
        return sum(self.suppression_values) / len(self.suppression_values) if self.suppression_values else 0.5

    @property
    def std_extractiveness(self) -> float:
        if len(self.extractiveness_values) < 2:
            return 0.0
        mean = self.avg_extractiveness
        variance = sum((x - mean) ** 2 for x in self.extractiveness_values) / len(self.extractiveness_values)
        return variance ** 0.5

    @property
    def std_suppression(self) -> float:
        if len(self.suppression_values) < 2:
            return 0.0
        mean = self.avg_suppression
        variance = sum((x - mean) ** 2 for x in self.suppression_values) / len(self.suppression_values)
        return variance ** 0.5

    @property
    def dominant_type(self) -> str:
        if not self.constraint_types:
            return 'unknown'
        return max(self.constraint_types.items(), key=lambda x: x[1])[0]

    @property
    def enforcement_ratio(self) -> float:
        total = self.enforcement_count + self.natural_count
        return self.enforcement_count / total if total > 0 else 0.5


def load_domain_registry() -> Dict[str, str]:
    """Load domain_category mappings from domain_registry.pl."""
    registry = {}
    if not DOMAIN_REGISTRY.exists():
        print(f"Warning: {DOMAIN_REGISTRY} not found")
        return registry

    with open(DOMAIN_REGISTRY, 'r', encoding='utf-8') as f:
        content = f.read()
        for match in DOMAIN_CATEGORY_REGEX.finditer(content):
            constraint_id, category = match.groups()
            registry[constraint_id] = category

    print(f"Loaded {len(registry)} domain mappings from registry")
    return registry


def parse_testset_file(filepath: Path, registry: Dict[str, str]) -> Optional[ConstraintData]:
    """Parse a single testset file and extract constraint data."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"Error reading {filepath}: {e}")
        return None

    # Extract constraint ID from interval or filename
    interval_match = INTERVAL_REGEX.search(content)
    if interval_match:
        constraint_id = interval_match.group(1)
        interval_start = int(interval_match.group(2))
        interval_end = int(interval_match.group(3))
    else:
        constraint_id = filepath.stem
        interval_start = None
        interval_end = None

    data = ConstraintData(
        constraint_id=constraint_id,
        file_path=str(filepath),
        interval_start=interval_start,
        interval_end=interval_end
    )

    # Extract base_extractiveness
    ext_match = EXTRACTIVENESS_REGEX.search(content)
    if ext_match:
        data.base_extractiveness = float(ext_match.group(2))

    # Extract suppression_score
    sup_match = SUPPRESSION_REGEX.search(content)
    if sup_match:
        data.suppression_score = float(sup_match.group(2))

    # Extract constraint_claim type
    claim_match = CONSTRAINT_CLAIM_REGEX.search(content)
    if claim_match:
        data.constraint_type = claim_match.group(2)

    # Check enforcement/natural emergence
    if REQUIRES_ENFORCEMENT_REGEX.search(content):
        data.requires_enforcement = True
    if EMERGES_NATURALLY_REGEX.search(content):
        data.emerges_naturally = True

    # Get category from registry
    data.category = registry.get(constraint_id, registry.get(filepath.stem, 'unknown_novel'))

    # Extract additional metrics
    for match in CONSTRAINT_METRIC_REGEX.finditer(content):
        metric_name = match.group(2)
        metric_value = float(match.group(3))
        data.metrics[metric_name] = metric_value

    return data


def analyze_corpus(testsets_dir: Path, registry: Dict[str, str]) -> Tuple[List[ConstraintData], Dict[str, CategoryStats]]:
    """Analyze all testset files and compute category statistics."""
    constraints = []
    category_stats: Dict[str, CategoryStats] = defaultdict(lambda: CategoryStats(category='unknown'))

    pl_files = sorted(testsets_dir.glob('*.pl'))
    print(f"Found {len(pl_files)} testset files")

    for filepath in pl_files:
        data = parse_testset_file(filepath, registry)
        if data is None:
            continue

        constraints.append(data)

        # Update category stats
        cat = data.category or 'unknown_novel'
        if cat not in category_stats:
            category_stats[cat] = CategoryStats(category=cat)

        stats = category_stats[cat]
        stats.count += 1

        if data.base_extractiveness is not None:
            stats.extractiveness_values.append(data.base_extractiveness)
        if data.suppression_score is not None:
            stats.suppression_values.append(data.suppression_score)
        if data.constraint_type:
            stats.constraint_types[data.constraint_type] += 1
        if data.requires_enforcement:
            stats.enforcement_count += 1
        if data.emerges_naturally:
            stats.natural_count += 1

    print(f"Parsed {len(constraints)} constraints across {len(category_stats)} categories")
    return constraints, dict(category_stats)


def compute_type_profiles(constraints: List[ConstraintData]) -> Dict[str, Dict[str, float]]:
    """Compute average profiles by constraint type (mountain/rope/noose/tangled_rope)."""
    type_stats = defaultdict(lambda: {'extractiveness': [], 'suppression': [], 'count': 0})

    for c in constraints:
        if c.constraint_type:
            type_stats[c.constraint_type]['count'] += 1
            if c.base_extractiveness is not None:
                type_stats[c.constraint_type]['extractiveness'].append(c.base_extractiveness)
            if c.suppression_score is not None:
                type_stats[c.constraint_type]['suppression'].append(c.suppression_score)

    profiles = {}
    for ctype, data in type_stats.items():
        ext_vals = data['extractiveness']
        sup_vals = data['suppression']
        profiles[ctype] = {
            'count': data['count'],
            'avg_extractiveness': sum(ext_vals) / len(ext_vals) if ext_vals else 0.5,
            'avg_suppression': sum(sup_vals) / len(sup_vals) if sup_vals else 0.5,
            'std_extractiveness': (sum((x - sum(ext_vals)/len(ext_vals))**2 for x in ext_vals) / len(ext_vals))**0.5 if len(ext_vals) > 1 else 0.0,
            'std_suppression': (sum((x - sum(sup_vals)/len(sup_vals))**2 for x in sup_vals) / len(sup_vals))**0.5 if len(sup_vals) > 1 else 0.0
        }

    return profiles


def infer_subcategories(constraints: List[ConstraintData], category_stats: Dict[str, CategoryStats]) -> Dict[str, Dict[str, float]]:
    """Infer subcategories based on clustering within categories."""
    subcategories = {}

    for cat, stats in category_stats.items():
        if stats.count < 5:
            continue

        # Split by constraint type within category
        type_groups = defaultdict(list)
        for c in constraints:
            if c.category == cat and c.constraint_type:
                type_groups[c.constraint_type].append(c)

        for ctype, group in type_groups.items():
            if len(group) >= 3:
                ext_vals = [c.base_extractiveness for c in group if c.base_extractiveness is not None]
                sup_vals = [c.suppression_score for c in group if c.suppression_score is not None]

                subcat_name = f"{cat}_{ctype}"
                subcategories[subcat_name] = {
                    'parent': cat,
                    'constraint_type': ctype,
                    'count': len(group),
                    'avg_extractiveness': sum(ext_vals) / len(ext_vals) if ext_vals else 0.5,
                    'avg_suppression': sum(sup_vals) / len(sup_vals) if sup_vals else 0.5
                }

    return subcategories


def generate_expanded_priors(
    category_stats: Dict[str, CategoryStats],
    type_profiles: Dict[str, Dict[str, float]],
    subcategories: Dict[str, Dict[str, float]]
) -> str:
    """Generate expanded domain_priors.pl content."""

    lines = []
    lines.append(":- module(domain_priors_expanded, [")
    lines.append("    get_corpus_prior/3,")
    lines.append("    category_corpus_profile/2,")
    lines.append("    type_corpus_profile/2,")
    lines.append("    subcategory_profile/2,")
    lines.append("    default_extractiveness/2,")
    lines.append("    default_suppression/2,")
    lines.append("    default_resistance/2,")
    lines.append("    infer_category_defaults/4")
    lines.append("]).")
    lines.append("")
    lines.append("/**")
    lines.append(" * DOMAIN PRIORS EXPANSION - Auto-generated from corpus analysis")
    lines.append(f" * Generated from {sum(s.count for s in category_stats.values())} constraints")
    lines.append(f" * Categories analyzed: {len(category_stats)}")
    lines.append(" *")
    lines.append(" * This module provides corpus-derived defaults for domain priors.")
    lines.append(" * Use these when a new domain lacks explicit priors.")
    lines.append(" */")
    lines.append("")

    # Category profiles derived from corpus
    lines.append("%% ============================================================================")
    lines.append("%% 1. CATEGORY CORPUS PROFILES (From Corpus Averages)")
    lines.append("%% ============================================================================")
    lines.append("%% Format: category_corpus_profile(Category, [AvgExtract, AvgSuppress, StdExtract, StdSuppress, Count]).")
    lines.append("")

    for cat, stats in sorted(category_stats.items(), key=lambda x: -x[1].count):
        profile = [
            round(stats.avg_extractiveness, 3),
            round(stats.avg_suppression, 3),
            round(stats.std_extractiveness, 3),
            round(stats.std_suppression, 3),
            stats.count
        ]
        dominant = stats.dominant_type
        enf_ratio = round(stats.enforcement_ratio, 2)
        lines.append(f"category_corpus_profile({cat}, {profile}).  % dominant: {dominant}, enforcement_ratio: {enf_ratio}")

    lines.append("")

    # Type profiles (mountain, rope, noose, tangled_rope)
    lines.append("%% ============================================================================")
    lines.append("%% 2. CONSTRAINT TYPE PROFILES (Mountain/Rope/Noose/Tangled Rope)")
    lines.append("%% ============================================================================")
    lines.append("%% Format: type_corpus_profile(Type, [AvgExtract, AvgSuppress, StdExtract, StdSuppress, Count]).")
    lines.append("")

    for ctype, profile in sorted(type_profiles.items(), key=lambda x: -x[1]['count']):
        vec = [
            round(profile['avg_extractiveness'], 3),
            round(profile['avg_suppression'], 3),
            round(profile['std_extractiveness'], 3),
            round(profile['std_suppression'], 3),
            profile['count']
        ]
        lines.append(f"type_corpus_profile({ctype}, {vec}).")

    lines.append("")

    # Subcategory profiles
    if subcategories:
        lines.append("%% ============================================================================")
        lines.append("%% 3. SUBCATEGORY PROFILES (Category + Type combinations)")
        lines.append("%% ============================================================================")
        lines.append("%% Format: subcategory_profile(SubcatName, [Parent, Type, AvgExtract, AvgSuppress, Count]).")
        lines.append("")

        for subcat, profile in sorted(subcategories.items(), key=lambda x: -x[1]['count']):
            vec = [
                profile['parent'],
                profile['constraint_type'],
                round(profile['avg_extractiveness'], 3),
                round(profile['avg_suppression'], 3),
                profile['count']
            ]
            lines.append(f"subcategory_profile({subcat}, {vec}).")

        lines.append("")

    # Default value inference predicates
    lines.append("%% ============================================================================")
    lines.append("%% 4. DEFAULT VALUE INFERENCE PREDICATES")
    lines.append("%% ============================================================================")
    lines.append("")
    lines.append("%% default_extractiveness(+Category, -Value)")
    lines.append("%% Returns corpus-derived default extractiveness for a category.")
    lines.append("default_extractiveness(Category, Value) :-")
    lines.append("    category_corpus_profile(Category, [Value|_]), !.")
    lines.append("default_extractiveness(_, 0.5).  % Neutral fallback")
    lines.append("")
    lines.append("%% default_suppression(+Category, -Value)")
    lines.append("%% Returns corpus-derived default suppression for a category.")
    lines.append("default_suppression(Category, Value) :-")
    lines.append("    category_corpus_profile(Category, [_, Value|_]), !.")
    lines.append("default_suppression(_, 0.5).  % Neutral fallback")
    lines.append("")
    lines.append("%% default_resistance(+Category, -Value)")
    lines.append("%% Infers resistance from extractiveness (inverse correlation).")
    lines.append("default_resistance(Category, Value) :-")
    lines.append("    default_extractiveness(Category, Ext),")
    lines.append("    Value is max(0.1, min(0.9, 1.0 - Ext * 0.5)), !.")
    lines.append("default_resistance(_, 0.5).")
    lines.append("")

    # Main inference predicate
    lines.append("%% infer_category_defaults(+Category, -Extractiveness, -Suppression, -Resistance)")
    lines.append("%% Unified predicate to get all defaults for a category.")
    lines.append("infer_category_defaults(Category, Ext, Sup, Res) :-")
    lines.append("    default_extractiveness(Category, Ext),")
    lines.append("    default_suppression(Category, Sup),")
    lines.append("    default_resistance(Category, Res).")
    lines.append("")

    # Type-based inference
    lines.append("%% get_corpus_prior(+ID, +Metric, -Value)")
    lines.append("%% Retrieves corpus-derived prior by constraint type.")
    lines.append("get_corpus_prior(ID, extractiveness, Value) :-")
    lines.append("    narrative_ontology:constraint_claim(ID, Type),")
    lines.append("    type_corpus_profile(Type, [Value|_]), !.")
    lines.append("get_corpus_prior(ID, suppression, Value) :-")
    lines.append("    narrative_ontology:constraint_claim(ID, Type),")
    lines.append("    type_corpus_profile(Type, [_, Value|_]), !.")
    lines.append("get_corpus_prior(_, _, 0.5).  % Neutral fallback")
    lines.append("")

    # Recommended category profile vectors (4-element format matching original domain_priors)
    lines.append("%% ============================================================================")
    lines.append("%% 5. RECOMMENDED CATEGORY PROFILE VECTORS")
    lines.append("%% ============================================================================")
    lines.append("%% These are corpus-calibrated replacements for the original category_profile/2.")
    lines.append("%% Format: [accessibility_collapse, stakes_inflation, suppression, resistance]")
    lines.append("")

    for cat, stats in sorted(category_stats.items(), key=lambda x: -x[1].count):
        if stats.count >= 5:  # Only output for categories with sufficient data
            # Map corpus stats to 4-vector format:
            # accessibility_collapse ~ inverse of enforcement_ratio (natural = high accessibility)
            # stakes_inflation ~ extractiveness (high extract = high stakes)
            # suppression ~ suppression_score
            # resistance ~ inverse correlation with extractiveness
            acc_collapse = round(1.0 - stats.enforcement_ratio * 0.5, 2)
            stakes = round(stats.avg_extractiveness * 1.2, 2)  # Slight inflation
            stakes = min(1.0, stakes)
            suppress = round(stats.avg_suppression, 2)
            resist = round(max(0.1, 1.0 - stats.avg_extractiveness * 0.6), 2)

            lines.append(f"% recommended_profile({cat}, [{acc_collapse}, {stakes}, {suppress}, {resist}]).  % N={stats.count}, dominant={stats.dominant_type}")

    lines.append("")
    lines.append("%% ============================================================================")
    lines.append("%% END OF AUTO-GENERATED PRIORS")
    lines.append("%% ============================================================================")

    return '\n'.join(lines)


def print_corpus_report(
    constraints: List[ConstraintData],
    category_stats: Dict[str, CategoryStats],
    type_profiles: Dict[str, Dict[str, float]]
):
    """Print a detailed corpus analysis report."""
    print("\n" + "=" * 70)
    print("   DOMAIN PRIORS CORPUS ANALYSIS REPORT")
    print("=" * 70)

    print(f"\nTotal constraints analyzed: {len(constraints)}")
    print(f"Categories found: {len(category_stats)}")

    # Category breakdown
    print("\n--- CATEGORY BREAKDOWN ---")
    print(f"{'Category':<30} {'Count':>6} {'Avg Ext':>8} {'Avg Sup':>8} {'Dominant':>12}")
    print("-" * 70)

    for cat, stats in sorted(category_stats.items(), key=lambda x: -x[1].count):
        print(f"{cat:<30} {stats.count:>6} {stats.avg_extractiveness:>8.3f} {stats.avg_suppression:>8.3f} {stats.dominant_type:>12}")

    # Type breakdown
    print("\n--- CONSTRAINT TYPE BREAKDOWN ---")
    print(f"{'Type':<15} {'Count':>6} {'Avg Ext':>8} {'Avg Sup':>8} {'Std Ext':>8} {'Std Sup':>8}")
    print("-" * 60)

    for ctype, profile in sorted(type_profiles.items(), key=lambda x: -x[1]['count']):
        print(f"{ctype:<15} {profile['count']:>6} {profile['avg_extractiveness']:>8.3f} {profile['avg_suppression']:>8.3f} {profile['std_extractiveness']:>8.3f} {profile['std_suppression']:>8.3f}")

    # Recommended defaults
    print("\n--- RECOMMENDED DEFAULTS FOR NEW DOMAINS ---")
    print("\nWhen inputting a new domain without explicit priors, use these corpus-derived defaults:")
    print()

    for cat, stats in sorted(category_stats.items(), key=lambda x: -x[1].count)[:5]:
        print(f"  {cat}:")
        print(f"    base_extractiveness: {stats.avg_extractiveness:.2f} (+/- {stats.std_extractiveness:.2f})")
        print(f"    suppression_score:   {stats.avg_suppression:.2f} (+/- {stats.std_suppression:.2f})")
        print(f"    dominant_type:       {stats.dominant_type}")
        print()

    print("=" * 70)


def save_stats_json(
    constraints: List[ConstraintData],
    category_stats: Dict[str, CategoryStats],
    type_profiles: Dict[str, Dict[str, float]],
    subcategories: Dict[str, Dict[str, float]]
):
    """Save corpus statistics to JSON for external analysis."""
    stats = {
        'summary': {
            'total_constraints': len(constraints),
            'categories': len(category_stats),
            'constraint_types': len(type_profiles)
        },
        'categories': {
            cat: {
                'count': s.count,
                'avg_extractiveness': round(s.avg_extractiveness, 4),
                'avg_suppression': round(s.avg_suppression, 4),
                'std_extractiveness': round(s.std_extractiveness, 4),
                'std_suppression': round(s.std_suppression, 4),
                'dominant_type': s.dominant_type,
                'enforcement_ratio': round(s.enforcement_ratio, 4),
                'type_distribution': dict(s.constraint_types)
            }
            for cat, s in category_stats.items()
        },
        'type_profiles': type_profiles,
        'subcategories': subcategories,
        'constraints': [
            {
                'id': c.constraint_id,
                'category': c.category,
                'type': c.constraint_type,
                'extractiveness': c.base_extractiveness,
                'suppression': c.suppression_score,
                'requires_enforcement': c.requires_enforcement,
                'emerges_naturally': c.emerges_naturally
            }
            for c in constraints
        ]
    }

    STATS_OUTPUT.parent.mkdir(exist_ok=True)
    with open(STATS_OUTPUT, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2)

    print(f"\nSaved detailed statistics to {STATS_OUTPUT}")


def main():
    print("Domain Priors Expander v1.0")
    print("-" * 40)

    # Load domain registry
    registry = load_domain_registry()

    # Analyze corpus
    constraints, category_stats = analyze_corpus(TESTSETS_DIR, registry)

    if not constraints:
        print("Error: No constraints found. Check TESTSETS_DIR path.")
        return

    # Compute type profiles
    type_profiles = compute_type_profiles(constraints)

    # Infer subcategories
    subcategories = infer_subcategories(constraints, category_stats)

    # Print report
    print_corpus_report(constraints, category_stats, type_profiles)

    # Generate expanded priors
    expanded_content = generate_expanded_priors(category_stats, type_profiles, subcategories)

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(expanded_content)

    print(f"\nGenerated expanded priors: {OUTPUT_FILE}")

    # Save JSON stats
    save_stats_json(constraints, category_stats, type_profiles, subcategories)

    print("\nDone!")


if __name__ == "__main__":
    main()
import subprocess
import os
import re
import google.generativeai as genai
from anthropic import Anthropic

# --- CONFIGURATION ---
PROLOG_PATH = "swipl" # Path to your SWI-Prolog executable
HARNESS_FILE = "test_harness.pl"
OUTPUT_DATA_FILE = "generated_dataset.pl"
MAX_RETRIES = 3

# Initialize APIs (Use your keys here)
genai.configure(api_key="YOUR_GEMINI_API_KEY")
anthropic = Anthropic(api_key="YOUR_CLAUDE_API_KEY")

def call_llm(prompt, model_type="gemini"):
    """Fetches Prolog data from the chosen LLM."""
    if model_type == "gemini":
        model = genai.GenerativeModel('gemini-1.5-pro')
        response = model.generate_content(prompt)
        return response.text
    elif model_type == "claude":
        message = anthropic.messages.create(
            model="claude-3-5-sonnet-20240620",
            max_tokens=4096,
            messages=[{"role": "user", "content": prompt}]
        )
        return message.content[0].text

def run_prolog_validation(interval_id):
    """
    Executes the Prolog test harness.
    Returns (Success Bool, Output String)
    """
    # Command: Load harness, run all tests for the interval, and halt.
    cmd = [
        PROLOG_PATH,
        "-q", "-g",
        f"consult('{HARNESS_FILE}'), run_all_tests({interval_id}), halt.",
        "-t", "halt(1)."
    ]

    result = subprocess.run(cmd, capture_output=True, text=True)
    return "ERROR" not in result.stdout and "[FAIL]" not in result.stdout, result.stdout

def clean_prolog_code(raw_text):
    """Extracts code blocks from LLM markdown response."""
    code_blocks = re.findall(r"```prolog\n(.*?)\n```", raw_text, re.DOTALL)
    return "\n".join(code_blocks) if code_blocks else raw_text

def orchestrate_audit(domain_topic, interval_id):
    print(f"\n[1] Initiating Audit for: {domain_topic}")

    # Load your prompt.md instructions
    with open("prompt.md", "r") as f:
        system_instructions = f.read()

    current_prompt = f"{system_instructions}\n\nTOPIC: {domain_topic}\nINTERVAL_ID: {interval_id}"

    for attempt in range(MAX_RETRIES):
        print(f"[Attempt {attempt + 1}] Generating Dataset...")
        raw_response = call_llm(current_prompt, model_type="gemini")
        prolog_code = clean_prolog_code(raw_response)

        # Write to file for Prolog to ingest
        with open(OUTPUT_DATA_FILE, "w") as f:
            f.write(prolog_code)

        # Validate using your stack
        success, logs = run_prolog_validation(interval_id)

        if success:
            print("[SUCCESS] Dataset validated and audited.")
            print(logs) # This contains your Executive Summary
            return True
        else:
            print("[!] Validation failed. Initiating self-correction loop...")
            # Extract the error messages to feed back to the LLM
            errors = "\n".join([line for line in logs.split('\n') if "ERROR" in line or "[FAIL]" in line])
            current_prompt = f"Your previous Prolog output had errors:\n{errors}\n\nPlease fix the predicates and ensure arity matches narrative_ontology.pl."

    print("[FAILURE] Could not generate a valid dataset after maximum retries.")
    return False

# --- EXECUTION ---
if __name__ == "__main__":
    # Example: Auditing a new domain
    orchestrate_audit("The 1986 Chernobyl Exclusion Zone Logic", "chernobyl_audit")
#!/usr/bin/env python3
"""
Corpus Data Extractor - Parses both output.txt and raw .pl files

Combines:
- Diagnostic output from Prolog engine (output.txt)
- Raw structural data from .pl files
- Merges into unified constraint objects
"""

import re
import json
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Optional, Tuple

class ConstraintData:
    """Unified constraint data structure"""

    def __init__(self, constraint_id):
        self.constraint_id = constraint_id
        self.claimed_type = None
        self.domain = None
        self.human_readable = None

        # Structural metrics
        self.extractiveness = None
        self.suppression = None
        self.resistance = None
        self.emerges_naturally = None
        self.requires_enforcement = None

        # Beneficiary data
        self.beneficiaries = []
        self.victims = []

        # Classifications (multiple per constraint)
        self.classifications = []  # [(type, context), ...]

        # From output.txt analysis
        self.structural_signature = None
        self.is_constructed = None
        self.omegas = []

        # Calculated
        self.variance_ratio = None
        self.index_configs_count = None
        self.types_produced_count = None

    def to_dict(self):
        """Convert to dictionary for JSON serialization"""
        return {
            'constraint_id': self.constraint_id,
            'claimed_type': self.claimed_type,
            'domain': self.domain,
            'human_readable': self.human_readable,
            'metrics': {
                'extractiveness': self.extractiveness,
                'suppression': self.suppression,
                'resistance': self.resistance,
                'emerges_naturally': self.emerges_naturally,
                'requires_enforcement': self.requires_enforcement
            },
            'beneficiaries': self.beneficiaries,
            'victims': self.victims,
            'classifications': [
                {'type': t, 'context': ctx} for t, ctx in self.classifications
            ],
            'analysis': {
                'structural_signature': self.structural_signature,
                'is_constructed': self.is_constructed,
                'omegas': self.omegas,
                'variance_ratio': self.variance_ratio,
                'index_configs': self.index_configs_count,
                'types_produced': self.types_produced_count
            }
        }

class CorpusExtractor:
    """Main extraction engine"""

    def __init__(self, output_txt_path, testsets_dir):
        self.output_txt_path = Path(output_txt_path)
        self.testsets_dir = Path(testsets_dir)
        self.constraints = {}  # id -> ConstraintData

    def extract_all(self):
        """Run complete extraction pipeline"""
        print("Step 1: Parsing output.txt...")
        self.parse_output_txt()

        print("Step 2: Parsing raw .pl files...")
        self.parse_pl_files()

        print("Step 3: Calculating derived metrics...")
        self.calculate_variance()

        print("Step 4: Inferring missing data...")
        self.infer_domains()

        return self.constraints

    def parse_output_txt(self):
        """Extract diagnostic data from Prolog engine output"""
        if not self.output_txt_path.exists():
            print(f"Warning: {self.output_txt_path} not found")
            return

        with open(self.output_txt_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # Split into constraint blocks
        # Pattern: "Constraint: {id}" starts a block
        constraint_blocks = re.split(r'(?=Constraint:\s+\w+)', content)

        for block in constraint_blocks:
            if not block.strip():
                continue

            # Extract constraint ID
            id_match = re.search(r'Constraint:\s+(\w+)', block)
            if not id_match:
                continue

            constraint_id = id_match.group(1)

            # Get or create constraint data
            if constraint_id not in self.constraints:
                self.constraints[constraint_id] = ConstraintData(constraint_id)

            constraint = self.constraints[constraint_id]

            # Extract claimed type
            type_match = re.search(r'Claimed Type:\s+(\w+)', block)
            if type_match:
                constraint.claimed_type = type_match.group(1)

            # Extract perspectives/classifications
            perspectives_section = re.search(
                r'Perspectives:(.*?)(?=\n\n|\nConstraint:|\Z)',
                block,
                re.DOTALL
            )

            if perspectives_section:
                persp_text = perspectives_section.group(1)
                # Parse lines like: "- [context(...)]: type"
                for match in re.finditer(
                    r'-\s*\[([^\]]+)\]:\s*(\w+)',
                    persp_text
                ):
                    context_str = match.group(1)
                    constraint_type = match.group(2)
                    constraint.classifications.append((constraint_type, context_str))

            # Extract extractiveness
            extr_match = re.search(
                r'extractive_noose.*?Intensity:\s*([\d.]+)',
                block
            )
            if extr_match:
                constraint.extractiveness = float(extr_match.group(1))

            # Alternative extraction pattern
            extr_match2 = re.search(r'Base Extractiveness:\s*([\d.]+)', block)
            if extr_match2:
                constraint.extractiveness = float(extr_match2.group(1))

            # Extract suppression
            supp_match = re.search(r'Suppression Requirement:\s*([\d.]+)', block)
            if supp_match:
                constraint.suppression = float(supp_match.group(1))

            # Extract resistance
            resist_match = re.search(r'Resistance to Change:\s*([\d.]+)', block)
            if resist_match:
                constraint.resistance = float(resist_match.group(1))

            # Detect constructed constraint
            if 'CONSTRUCTED CONSTRAINT signature' in block:
                constraint.is_constructed = True
                constraint.structural_signature = 'constructed'
            elif 'natural_constraint' in block.lower():
                constraint.is_constructed = False
                constraint.structural_signature = 'natural'

            # Extract omegas
            for omega_match in re.finditer(r'Î©:\s+(omega_\w+)\s+\((\w+)\)', block):
                omega_id = omega_match.group(1)
                omega_type = omega_match.group(2)
                constraint.omegas.append({'id': omega_id, 'type': omega_type})

    def parse_pl_files(self):
        """Extract structured data from raw .pl files"""
        if not self.testsets_dir.exists():
            print(f"Warning: {self.testsets_dir} not found")
            return

        pl_files = list(self.testsets_dir.glob('*.pl'))
        print(f"  Found {len(pl_files)} .pl files")

        for filepath in pl_files:
            constraint_id = filepath.stem

            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
            except Exception as e:
                print(f"  Error reading {filepath.name}: {e}")
                continue

            # Get or create constraint
            if constraint_id not in self.constraints:
                self.constraints[constraint_id] = ConstraintData(constraint_id)

            constraint = self.constraints[constraint_id]

            # Extract human-readable description
            hr_match = re.search(r'human_readable:\s*["\']([^"\']+)["\']', content)
            if hr_match:
                constraint.human_readable = hr_match.group(1)

            # Extract domain
            domain_match = re.search(r'domain:\s*(\w+)', content)
            if domain_match:
                constraint.domain = domain_match.group(1)

            # Alternative domain extraction from category_of
            cat_match = re.search(
                r'domain_priors:category_of\([^,]+,\s*(\w+)\)',
                content
            )
            if cat_match and not constraint.domain:
                constraint.domain = cat_match.group(1)

            # Extract base_extractiveness
            extr_match = re.search(
                r'base_extractiveness\([^,]+,\s*([\d.]+)\)',
                content
            )
            if extr_match and not constraint.extractiveness:
                constraint.extractiveness = float(extr_match.group(1))

            # Extract suppression_score
            supp_match = re.search(
                r'suppression_score\([^,]+,\s*([\d.]+)\)',
                content
            )
            if supp_match and not constraint.suppression:
                constraint.suppression = float(supp_match.group(1))

            # Extract emerges_naturally
            if re.search(r'emerges_naturally\([^)]+\)', content):
                constraint.emerges_naturally = True
                constraint.requires_enforcement = False

            # Extract requires_active_enforcement
            if re.search(r'requires_active_enforcement\([^)]+\)', content):
                constraint.requires_enforcement = True
                constraint.emerges_naturally = False

            # Extract beneficiaries
            for ben_match in re.finditer(
                r'constraint_beneficiary\([^,]+,\s*(\w+)\)',
                content
            ):
                beneficiary = ben_match.group(1)
                if beneficiary not in constraint.beneficiaries:
                    constraint.beneficiaries.append(beneficiary)

            # Extract victims
            for vic_match in re.finditer(
                r'constraint_victim\([^,]+,\s*(\w+)\)',
                content
            ):
                victim = vic_match.group(1)
                if victim not in constraint.victims:
                    constraint.victims.append(victim)

            # Extract claimed type from constraint_claim
            claim_match = re.search(
                r'constraint_claim\([^,]+,\s*(\w+)\)',
                content
            )
            if claim_match and not constraint.claimed_type:
                constraint.claimed_type = claim_match.group(1)

            # Extract classifications
            # Pattern: constraint_classification(id, type, context(...))
            for class_match in re.finditer(
                r'constraint_classification\s*\(\s*[^,]+,\s*(\w+),\s*context\(([^)]+)\)',
                content,
                re.DOTALL
            ):
                constraint_type = class_match.group(1)
                context_params = class_match.group(2)

                # Parse context parameters
                # agent_power(X), time_horizon(Y), exit_options(Z), spatial_scope(W)
                context_dict = {}
                for param in ['agent_power', 'time_horizon', 'exit_options', 'spatial_scope']:
                    param_match = re.search(
                        rf'{param}\(([^)]+)\)',
                        context_params
                    )
                    if param_match:
                        context_dict[param] = param_match.group(1).strip()

                # Create context tuple for comparison
                context_tuple = (
                    context_dict.get('agent_power'),
                    context_dict.get('time_horizon'),
                    context_dict.get('exit_options'),
                    context_dict.get('spatial_scope')
                )

                constraint.classifications.append((constraint_type, context_tuple))

    def calculate_variance(self):
        """Calculate variance ratios for each constraint"""
        for constraint in self.constraints.values():
            if not constraint.classifications:
                constraint.variance_ratio = None
                constraint.index_configs_count = 0
                constraint.types_produced_count = 0
                continue

            # Get unique index configs
            unique_contexts = set()
            for _, context in constraint.classifications:
                if isinstance(context, tuple):
                    unique_contexts.add(context)
                else:
                    # String representation - count it
                    unique_contexts.add(context)

            constraint.index_configs_count = len(unique_contexts)

            # Get unique types produced
            unique_types = set(t for t, _ in constraint.classifications)
            constraint.types_produced_count = len(unique_types)

            # Calculate variance ratio
            if constraint.index_configs_count > 0:
                constraint.variance_ratio = (
                    constraint.types_produced_count / constraint.index_configs_count
                )
            else:
                constraint.variance_ratio = None

    def infer_domains(self):
        """Infer domains from constraint names if missing"""
        domain_patterns = {
            'technical': ['protocol', 'algorithm', 'standard', 'rfc', 'tcp', 'ip',
                         'http', 'ssl', 'api', 'code', 'software', 'hardware'],
            'social': ['law', 'policy', 'norm', 'social', 'cultural', 'community',
                      'tradition', 'custom', 'etiquette', 'convention'],
            'economic': ['market', 'tax', 'trade', 'price', 'economic', 'financial',
                        'currency', 'credit', 'investment', 'capital', 'interest'],
            'biological': ['gene', 'evolution', 'species', 'disease', 'biology',
                          'organism', 'cell', 'protein', 'dna', 'ecology'],
            'narrative': ['ulysses', 'story', 'character', 'stephen', 'bloom',
                         'molly', 'dublin', 'joyce', 'novel', 'chp'],
            'legal': ['statute', 'regulation', 'doctrine', 'court', 'judicial',
                     'constitutional', 'criminal', 'civil', 'tort', 'contract'],
            'physical': ['gravity', 'physics', 'thermodynamics', 'quantum',
                        'mechanics', 'electromagnetic', 'particle', 'wave'],
            'mathematical': ['theorem', 'proof', 'axiom', 'lemma', 'conjecture',
                           'paradox', 'equation', 'formula', 'geometric'],
            'formal_logic': ['logic', 'proof', 'axiom', 'theorem', 'tautology',
                           'contradiction', 'inference', 'deduction']
        }

        for constraint in self.constraints.values():
            if constraint.domain:
                continue

            # Check constraint ID against patterns
            constraint_lower = constraint.constraint_id.lower()

            scores = defaultdict(int)
            for domain, keywords in domain_patterns.items():
                for keyword in keywords:
                    if keyword in constraint_lower:
                        scores[domain] += 1

            if scores:
                # Pick highest scoring domain
                best_domain = max(scores.items(), key=lambda x: x[1])[0]
                constraint.domain = best_domain

    def save_json(self, output_path):
        """Save extracted data to JSON"""
        data = {
            'constraints': {
                cid: constraint.to_dict()
                for cid, constraint in self.constraints.items()
            },
            'summary': {
                'total_constraints': len(self.constraints),
                'with_extractiveness': sum(1 for c in self.constraints.values()
                                          if c.extractiveness is not None),
                'with_suppression': sum(1 for c in self.constraints.values()
                                       if c.suppression is not None),
                'with_classifications': sum(1 for c in self.constraints.values()
                                          if c.classifications),
                'with_domain': sum(1 for c in self.constraints.values()
                                  if c.domain is not None)
            }
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2)

        print(f"\nSaved to {output_path}")
        print(f"Summary:")
        for key, value in data['summary'].items():
            pct = (value / data['summary']['total_constraints'] * 100) if data['summary']['total_constraints'] > 0 else 0
            print(f"  {key}: {value} ({pct:.1f}%)")

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Extract corpus data from output.txt and .pl files'
    )
    parser.add_argument(
        '--output-txt',
        default='../outputs/output.txt',
        help='Path to output.txt'
    )
    parser.add_argument(
        '--testsets',
        default='../prolog/testsets/',
        help='Path to testsets directory'
    )
    parser.add_argument(
        '--json-output',
        default='../outputs/corpus_data.json',
        help='Output JSON file'
    )

    args = parser.parse_args()

    extractor = CorpusExtractor(args.output_txt, args.testsets)
    constraints = extractor.extract_all()

    print(f"\nExtracted {len(constraints)} constraints")

    extractor.save_json(args.json_output)

if __name__ == '__main__':
    main()
import re
import sys
from pathlib import Path

def parse_log_content(content):
    """
    Parses the log content to find and extract False Mountain details.
    """
    false_mountains = []
    
    # Split the entire log into chunks, each starting with a new scenario load.
    # The delimiter is a lookahead assertion to keep the delimiter string.
    scenario_chunks = re.split(r'(?=\[SCENARIO MANAGER\] Clearing Knowledge Base...)', content)

    for chunk in scenario_chunks:
        if not chunk.strip():
            continue

        # Get the name of the constraint being tested in this chunk
        name_match = re.search(r'Loading: \.\./prolog/testsets/(.+?)\.pl', chunk)
        if not name_match:
            continue
        constraint_name = name_match.group(1)

        # Split the scenario chunk by perspectival gaps, which are our triggers
        gap_chunks = re.split(r'(?=! ALERT:|! GAP:)', chunk)
        
        for gap_chunk in gap_chunks:
            if not gap_chunk.startswith('! ALERT:') and not gap_chunk.startswith('! GAP:'):
                continue
            
            fm_data = {
                'name': constraint_name,
                'severity': 'N/A',
                'powerless_view': 'N/A',
                'institutional_view': 'N/A',
                'resolution_strategy': ''
            }

            # Extract gap and perspectives
            gap_match = re.search(r'(! (?:ALERT|GAP): .+)', gap_chunk)
            if gap_match:
                fm_data['gap_detected'] = gap_match.group(1).strip()

            # More robust perspective extraction
            pv_match = re.search(r'(?:Individual \(Powerless\)|Powerless see): (\w+)', gap_chunk)
            if pv_match:
                fm_data['powerless_view'] = pv_match.group(1)

            iv_match = re.search(r'(?:Institutional \(Manager\)|Institutions see): (\w+)', gap_chunk)
            if iv_match:
                fm_data['institutional_view'] = iv_match.group(1)

            # Extract Omega
            omega_match = re.search(r'Î©: (.+)', gap_chunk)
            if omega_match:
                fm_data['omega_question'] = omega_match.group(1).strip()
            else:
                fm_data['omega_question'] = "N/A"

            # Extract Severity from the Triage section
            if '[critical]' in gap_chunk:
                fm_data['severity'] = 'critical'
            elif '[high]' in gap_chunk:
                fm_data['severity'] = 'high'
            
            # Extract Resolution Strategy
            res_match = re.search(r'RESOLUTION STRATEGY:\s*\n(.*?)(?:\n\s*â””â”€|\Z)', gap_chunk, re.DOTALL)
            if res_match:
                strategy = res_match.group(1).strip()
                # Clean up the left-aligned junk from the box drawing
                cleaned_strategy = "\n".join(line.strip().lstrip('â”‚').lstrip() for line in strategy.split('\n'))
                fm_data['resolution_strategy'] = cleaned_strategy

            # Add to list if we have the core info
            if 'gap_detected' in fm_data:
                false_mountains.append(fm_data)

    # Deduplicate entries based on constraint name and the gap detected
    unique_fms = []
    seen = set()
    for fm in false_mountains:
        identifier = (fm['name'], fm['gap_detected'])
        if identifier not in seen:
            unique_fms.append(fm)
            seen.add(identifier)

    return unique_fms

def generate_markdown_report(false_mountains, output_path):
    """
    Generates a Markdown report from the list of False Mountain data.
    """
    # Sort by severity (critical first), then by name
    sorted_fms = sorted(false_mountains, key=lambda x: (x['severity'] != 'critical', x['name']))

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# False Mountain Diagnostic Report\n\n")
        f.write(f"**Total Unique False Mountains Found:** {len(sorted_fms)}\n\n")
        f.write("---\n\n")

        for i, fm in enumerate(sorted_fms, 1):
            f.write(f"### {i}. False Mountain: `{fm['name']}`\n\n")
            f.write(f"*   **Severity:** `{fm['severity']}`\n")
            f.write(f"*   **Gap Detected:** {fm['gap_detected']}\n")
            
            if fm['powerless_view'] != 'N/A' or fm['institutional_view'] != 'N/A':
                f.write(f"*   **Perspectival Mismatch:**\n")
                if fm['powerless_view'] != 'N/A':
                    f.write(f"    *   **Powerless View:** `{fm['powerless_view']}`\n")
                if fm['institutional_view'] != 'N/A':
                    f.write(f"    *   **Institutional View:** `{fm['institutional_view']}`\n")
            
            f.write(f"*   **Generated Omega:** {fm['omega_question']}\n")
            f.write(f"*   **Suggested Resolution Strategy:**\n")
            f.write(f"    ```\n{fm['resolution_strategy']}\n    ```\n\n")
            f.write("---\n\n")

def main():
    """
    Main function to run the reporter.
    """
    # Using pathlib for more robust path handling
    script_dir = Path(__file__).parent
    log_file = script_dir / '../outputs/output.txt'
    report_file = script_dir / '../outputs/false_mountain_report.md'
    
    print("Parsing log file to find False Mountains...")
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            log_content = f.read()
    except FileNotFoundError:
        print(f"Error: Log file not found at {log_file}", file=sys.stderr)
        sys.exit(1)
        
    false_mountains_data = parse_log_content(log_content)
    
    if false_mountains_data:
        print(f"Found {len(false_mountains_data)} unique False Mountains.")
        print(f"Generating report at {report_file}...")
        generate_markdown_report(false_mountains_data, report_file)
        print("Report generated successfully.")
    else:
        print("No False Mountains found in the log file.")

if __name__ == '__main__':
    main()
import os
import re

# Standard mapping for non-standard ontologies found in your linter output
ONTOLOGY_MAP = {
    "organizational_decay": "snare",
    "election_cycle": "rope",
    "natural_law": "mountain",
    "physical_law": "mountain",
    "algorithmic_determinism": "mountain",
    "biomountain": "mountain"
}

def repair_file(filepath):
    with open(filepath, 'r') as f:
        content = f.read()

    filename = os.path.basename(filepath)
    domain_id = filename.replace(".pl", "")
    changed = False

    # 1. Map Illegal Ontologies
    for old, new in ONTOLOGY_MAP.items():
        if f", {old})" in content:
            content = re.sub(rf'constraint_claim\(([^,]+),\s*{old}\)', rf'constraint_claim(\1, {new})', content)
            changed = True

    # 2. Inject Missing Pillars (Indexical Relativity Stubs)
    pillars = ['mountain', 'rope', 'snare']
    missing = []
    for p in pillars:
        # Check for constraint_classification(ID, Pillar, ...)
        if not re.search(rf'constraint_classification\(\s*{domain_id}\s*,\s*{p}\s*,', content):
            missing.append(p)

    if missing:
        content += "\n\n% --- v3.1 Indexical Relativity Stubs (Fleet Repair) ---\n"
        for m in missing:
            power = "analytical" if m == 'mountain' else "institutional" if m == 'rope' else "individual_powerless"
            content += f"constraint_indexing:constraint_classification({domain_id}, {m}, agent_power({power})).\n"
        changed = True

    # 3. Resolve Mandatrophy Omegas (The "Indexical Relativity" Gate)
    ext_match = re.search(r'base_extractiveness\([^,]+,\s*([\d\.]+)\)', content)
    if ext_match:
        val = float(ext_match.group(1))
        if val > 0.7 and "[RESOLVED MANDATROPHY]" not in content:
            # Prepend resolution tag to the file header
            content = f"% [RESOLVED MANDATROPHY] High-extraction Mountain identified as structural mandate.\n" + content
            changed = True

    if changed:
        with open(filepath, 'w') as f:
            f.write(content)
        return True
    return False

if __name__ == "__main__":
    target_dir = "../prolog/testsets"
    repaired_count = 0
    for root, _, files in os.walk(target_dir):
        for file in files:
            if file.endswith('.pl'):
                if repair_file(os.path.join(root, file)):
                    repaired_count += 1
                    print(f"âœ“ Hardened: {file}")

    print(f"\n--- REPAIR COMPLETE: {repaired_count} Files Standardized ---")
import os
import re
import json

TESTSETS_DIR = "../prolog/testsets/"
ANALYSIS_FILE = "../outputs/structured_analysis.json"

def analyze_gaps():
    results = {}
    files = [f for f in os.listdir(TESTSETS_DIR) if f.endswith('.pl')]

    # Regex to find the classification type used in the indexing block
    type_regex = re.compile(r"constraint_classification\(\s*[^,]+,\s*([a-zA-Z0-9_]+),")

    print(f"{'DOMAIN':<45} | {'ROPE':<5} | {'NOOSE':<5} | {'MTN':<5} | {'STATUS'}")
    print("-" * 85)

    for filename in sorted(files):
        path = os.path.join(TESTSETS_DIR, filename)
        domain_id = filename.replace('.pl', '')

        with open(path, 'r') as f:
            content = f.read()
            found_types = type_regex.findall(content)

        # Check for the presence of each pillar
        has_rope = "rope" in found_types
        has_noose = "noose" in found_types
        has_mountain = "mountain" in found_types

        status = "COMPLETE" if (has_rope and has_noose and has_mountain) else "GAP"

        # Color/Visual indicators for terminal
        r_mark = "âœ“" if has_rope else "âœ—"
        n_mark = "âœ“" if has_noose else "âœ—"
        m_mark = "âœ“" if has_mountain else "âœ—"

        print(f"{domain_id:<45} |  {r_mark}   |  {n_mark}   |  {m_mark}   | {status}")

        results[domain_id] = {
            "rope": has_rope,
            "noose": has_noose,
            "mountain": has_mountain,
            "is_complete": status == "COMPLETE"
        }

    # Save to a report for LLM evaluation
    with open("../outputs/gap_report.json", "w") as out:
        json.dump(results, out, indent=4)

    summary = sum(1 for v in results.values() if v["is_complete"])
    print(f"\nâœ“ Analysis Complete: {summary}/{len(files)} files are perspectivally complete.")

if __name__ == "__main__":
    analyze_gaps()
import os
import re

# Mapping non-standard terms to the 3-pillar standard
FIXES = {
    'algorithmic_determinism': 'mountain',
    'biomountain': 'mountain',
    'logic_limit': 'mountain'
}

def harden_testsets(directory):
    resolved = 0
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith('.pl'):
                path = os.path.join(root, file)
                with open(path, 'r') as f:
                    content = f.read()

                new_content = content
                for old, new in FIXES.items():
                    # Specifically target constraint_claim declarations
                    pattern = rf'constraint_claim\(([^,]+),\s*{old}\)'
                    new_content = re.sub(pattern, rf'constraint_claim(\1, {new})', new_content)

                if content != new_content:
                    with open(path, 'w') as f:
                        f.write(new_content)
                    print(f"âœ“ Hardened: {file} ({list(FIXES.keys())})")
                    resolved += 1
    return resolved

if __name__ == "__main__":
    # Adjust path if your testsets are located elsewhere
    testset_dir = "../prolog/testsets"
    if os.path.exists(testset_dir):
        count = harden_testsets(testset_dir)
        print(f"\n--- SUCCESS: {count} Files Standardized ---")
    else:
        print(f"Error: Directory {testset_dir} not found.")
# python3 clean_log.py audit.log > signal_only.txt


import sys
import re

def extract_signal(log_content):
    # Regex patterns for key signal elements
    domain_pattern = re.compile(r'^\[\d+\] DOMAIN:.*')
    omega_pattern = re.compile(r'^\s*- Î©_.*')
    summary_delimiter = "===================================================="
    error_keywords = ["ERROR:", "[FAIL]", "Syntax error", "Warning:"]
    final_tally = re.compile(r'^DONE: \d+ Passed, \d+ Failed')

    signal = []
    in_summary_block = False

    lines = log_content.splitlines()

    for line in lines:
        clean_line = line.strip()

        # 1. Capture Domain Headers
        if domain_pattern.match(clean_line):
            signal.append(f"\n{clean_line}")
            continue

        # 2. Capture Errors and Redefinition Warnings
        if any(kw in clean_line for kw in error_keywords):
            signal.append(f"  >>> SIGNAL_ALERT: {clean_line}")
            continue

        # 3. Capture Omega Variables (The conceptual/empirical pivots)
        if omega_pattern.match(line): # use original line to preserve indentation
            signal.append(line.rstrip())
            continue

        # 4. Capture Executive Summary Blocks (The "Meat")
        if summary_delimiter in clean_line:
            in_summary_block = not in_summary_block
            signal.append(summary_delimiter)
            continue

        if in_summary_block:
            signal.append(line.rstrip())
            continue

        # 5. Capture the final status tally
        if final_tally.match(clean_line):
            signal.append(f"\n{clean_line}")

    return "\n".join(signal)

if __name__ == "__main__":
    # If you save your log to 'audit.log', run: python script.py audit.log
    if len(sys.argv) > 1:
        with open(sys.argv[1], 'r', encoding='utf-8') as f:
            content = f.read()
    else:
        # Fallback for demonstration: read from stdin
        content = sys.stdin.read()

    print(extract_signal(content))
import subprocess
import os
import google.generativeai as genai
from anthropic import Anthropic

# --- SYSTEM SETUP ---
PROLOG_EXECUTABLE = "swipl"
HARNESS_FILE = "test_harness.pl"
TEMP_DATA_FILE = "automated_audit_data.pl"

class DRAuditOrchestrator:
    def __init__(self, gemini_key, claude_key):
        genai.configure(api_key=gemini_key)
        self.claude = Anthropic(api_key=claude_key)
        self.gemini = genai.GenerativeModel('gemini-1.5-pro')

    def pre_scan_omegas(self, source_text):
        """Step 1: Identify Reasoning Blockers (Omegas) before code generation."""
        prompt = f"""Analyze this text for Deferential Realism (DR) Omega Variables:
        - Empirical (missing data)
        - Conceptual (ambiguous terms)
        - Preference (value judgments)

        TEXT: {source_text}
        Output ONLY a list of omega_variable(ID, Type, Description) predicates."""

        response = self.gemini.generate_content(prompt)
        return response.text

    def generate_dataset(self, source_text, omegas, interval_id):
        """Step 2: Generate the full Prolog dataset using the predefined Omegas."""
        with open("prompt.md", "r") as f:
            system_prompt = f.read()

        full_prompt = f"{system_prompt}\n\nUSE THESE OMEGAS:\n{omegas}\n\nSOURCE TEXT:\n{source_text}\nINTERVAL_ID: {interval_id}"

        # Use Claude for high-precision Prolog syntax
        message = self.claude.messages.create(
            model="claude-3-5-sonnet-20240620",
            max_tokens=4000,
            messages=[{"role": "user", "content": full_prompt}]
        )
        return message.content[0].text

    def run_prolog_audit(self, interval_id):
        """Step 3 & 4: Execute the Prolog validation and catch specific DRL errors."""
        # Use -q for quiet and -g to call the test harness
        cmd = [
            PROLOG_EXECUTABLE, "-q", "-t", "halt", "-g",
            f"consult('{HARNESS_FILE}'), run_all_tests({interval_id}), halt."
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        return result.stdout

    def run_audit_cycle(self, topic_text, interval_id):
        print(f"[*] Starting DR-Audit for {interval_id}...")

        # 1. Pre-Scan Omegas
        omegas = self.pre_scan_omegas(topic_text)

        # 2. Initial Generation
        prolog_code = self.generate_dataset(topic_text, omegas, interval_id)

        # Clean markdown wrappers if present
        prolog_code = prolog_code.replace("```prolog", "").replace("```", "")

        with open(TEMP_DATA_FILE, "w") as f:
            f.write(prolog_code)

        # 3. Validate and Refine (Loop if necessary)
        logs = self.run_prolog_audit(interval_id)

        if "! ALERT" in logs or "ERROR" in logs:
            print("[!] Ontological Fraud or Schema Error detected. Refining...")
            # Extract error lines to provide to the LLM for correction
            error_feedback = "\n".join([l for l in logs.split("\n") if "ALERT" in l or "ERROR" in l])
            # Re-run generate_dataset with error_feedback as a correction prompt...
            # (Loop logic would go here)

        print("[*] Final Executive Summary Generated:")
        print(logs)

# --- INITIATION ---
# orchestrator = DRAuditOrchestrator(GEMINI_KEY, CLAUDE_KEY)
# orchestrator.run_audit_cycle(source_document_text, "audit_2026_chernobyl")
#!/usr/bin/env python3
import re
import sys
import os
from collections import defaultdict, Counter
from pathlib import Path

# Construct absolute paths
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(SCRIPT_DIR)

class MetaReporter:
    def __init__(self, output_file):
        self.output_file = Path(output_file)
        self.omegas = []
        self.constraint_types = Counter()
        self.passed_tests = 0
        self.failed_tests = 0
        self.test_failures = []
        self.total_constraints_analyzed = 0 # New field for overall count

    def parse_output(self):
        if not self.output_file.exists():
            print(f"Error: {self.output_file} does not exist")
            return False

        with open(self.output_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # --- Global Stats ---
        self.passed_tests = len(re.findall(r'\[PASS\]', content))
        self.failed_tests = len(re.findall(r'\[(AUDIT FAIL|FAIL)\]', content))
        self.total_constraints_analyzed = self.passed_tests + self.failed_tests # Total from execution summary
        
        # Regex to find failure summary lines.
        for match in re.finditer(r"  -\s\[(\w+)\]\s(.+?)\n\s+Reason:\s(.+)", content):
            ftype, path, detail = match.groups()
            self.test_failures.append({'type': ftype, 'path': path.strip(), 'detail': detail.strip()})
        
        # --- Scenario-Specific Stats ---
        scenario_chunks = re.split(r'(?=\[SCENARIO MANAGER\] Clearing Knowledge Base\.\.\.)', content)

        for chunk in scenario_chunks:
            if not chunk.strip():
                continue

            # Extract constraint type from this chunk (more flexible regex)
            ctype_match = re.search(r'Claimed Type:\s*(\w+)', chunk)
            if ctype_match:
                self.constraint_types[ctype_match.group(1)] += 1
            
            # Find omegas (more flexible regex for the preceding "Î©:" and type)
            for match in re.finditer(r'Î©:\s*omega_\w+_(\w+)\s*\((conceptual|empirical|preference)\)', chunk): # Removed trailing ':'
                constraint, omega_type = match.groups()
                if (constraint, omega_type) not in self.omegas:
                    self.omegas.append((constraint, omega_type))
        return True

    def generate_report(self):
        print("\n" + "="*80)
        print("META-REPORT: Test Suite Analysis".center(80))
        print("="*80 + "\n")
        self._section_test_results()
        self._section_corpus_overview()
        self._section_omegas()
        print("="*80 + "\n")

    def _section_test_results(self):
        print("ðŸ“Š TEST EXECUTION SUMMARY")
        print("-" * 80)
        total = self.passed_tests + self.failed_tests
        if total > 0:
            pass_rate = (self.passed_tests / total) * 100 if total > 0 else 0
            print(f"  Total Tests Run: {total}")
            print(f"  Passed:          {self.passed_tests} ({pass_rate:.1f}%)")
            print(f"  Failed:          {self.failed_tests}")
            if self.failed_tests > 0:
                print(f"  Failure Details:")
                for failure in self.test_failures:
                    print(f"    - [{failure['type']}] {failure['path']}")
                    print(f"      Reason: {failure['detail'][:100]}")
        else:
            print("  No test results found")
        print()

    def _section_corpus_overview(self):
        print("ðŸ“š CORPUS OVERVIEW")
        print("-" * 80)
        print(f"  Total Constraints Analyzed: {self.total_constraints_analyzed}")
        if self.constraint_types:
            print(f"  Identified Constraint Type Distribution (from test output):")
            for ctype, count in self.constraint_types.most_common():
                percentage = (count / self.total_constraints_analyzed) * 100 if self.total_constraints_analyzed > 0 else 0
                print(f"    - {ctype:15s}: {count:4d} ({percentage:.1f}%)")
        else:
            print("  No explicit constraint types identified in test output.")
        print()

    def _section_omegas(self):
        print("â“ EPISTEMOLOGICAL GAPS (Omegas)")
        print("-" * 80)
        if self.omegas:
            print(f"  Total Unique Omegas Found: {len(self.omegas)}")
            omega_types = Counter(o_type for _, o_type in self.omegas)
            for omega_type, count in omega_types.most_common():
                print(f"  - {omega_type.upper()}: {count}")
        else:
            print(f"  âœ“ No unresolved omegas detected")
        print()

def main():
    output_file = os.path.join(ROOT_DIR, 'outputs', 'output.txt')
    reporter = MetaReporter(output_file)
    if reporter.parse_output():
        reporter.generate_report()
    else:
        sys.exit(1)

if __name__ == '__main__':
    main()import re

class NarrativeContextHandler:
    def __init__(self, raw_text):
        self.raw_text = raw_text
        self.intervals = []

    def chunk_by_narrative(self, anchor_pattern=r"(Chapter \d+|Section \d+|ARTICLE [IVX]+)"):
        """
        Chunks the document based on structural headers to preserve
        ontological coherence within each interval.
        """
        # Identify start positions of narrative headers
        splits = [m.start() for m in re.finditer(anchor_pattern, self.raw_text)]
        if not splits:
            # Fallback to a mid-point narrative break if no headers found
            splits = [0, len(self.raw_text) // 2]

        splits.append(len(self.raw_text))

        for i in range(len(splits) - 1):
            chunk = self.raw_text[splits[i]:splits[i+1]]
            interval_id = f"interval_{i}"
            # Define T0 and Tn as relative integers for Prolog temporal integrity
            self.intervals.append({
                "id": interval_id,
                "text": chunk.strip(),
                "t_range": (i * 10, (i + 1) * 10) # Maps to integer requirements
            })
        return self.intervals

    def map_to_prolog_intervals(self):
        """Generates the interval/3 predicates for the ontology."""
        predicates = []
        for item in self.intervals:
            t0, tn = item["t_range"]
            predicates.append(f"interval({item['id']}, {t0}, {tn}).")
        return "\n".join(predicates)
import re
import sys
from pathlib import Path

def parse_log_content(content):
    """
    Parses the log content to find and extract all Omega details.
    """
    omegas = []
    
    scenario_chunks = re.split(r'(?=\[SCENARIO MANAGER\] Clearing Knowledge Base...)', content)

    for chunk in scenario_chunks:
        if not chunk.strip() or 'Î©:' not in chunk:
            continue

        name_match = re.search(r'Loading: \.\./prolog/testsets/(.+?)\.pl', chunk)
        if not name_match:
            continue
        constraint_name = name_match.group(1)

        # Split the scenario chunk by Omega Generation sections
        omega_gen_sections = re.split(r'(?=\[OMEGA GENERATION FROM PERSPECTIVAL GAPS:)', chunk)

        for omega_chunk in omega_gen_sections:
            if 'Î©:' not in omega_chunk:
                continue

            omega_data = {
                'name': 'N/A',
                'severity': 'N/A',
                'associated_constraint': constraint_name,
                'source_gap': 'N/A',
                'question': 'N/A',
                'resolution_strategy': ''
            }

            # Extract Omega variable name, question, and source
            omega_name_match = re.search(r'Î©:\s*(\w+)', omega_chunk)
            if omega_name_match:
                omega_data['name'] = omega_name_match.group(1)

            omega_question_match = re.search(r'Question:\s*(.+)', omega_chunk)
            if omega_question_match:
                omega_data['question'] = omega_question_match.group(1).strip()
            
            source_gap_match = re.search(r'Source:\s*(.+)', omega_chunk)
            if source_gap_match:
                omega_data['source_gap'] = source_gap_match.group(1).strip()

            # Find the corresponding Triage and Resolution sections
            triage_chunk = omega_chunk
            # To get the right triage/resolution, we need to search in the remainder of the chunk
            if omega_data['name'] != 'N/A':
                # Narrow down the search area to after the omega name
                search_area_match = re.search(re.escape(omega_data['name']) + r'(.*)', omega_chunk, re.DOTALL)
                if search_area_match:
                    search_area = search_area_match.group(1)
                    
                    triage_section_match = re.search(r'\[OMEGA TRIAGE & PRIORITIZATION\](.*?)(?=\n\n|\n\[OMEGA RESOLUTION)', search_area, re.DOTALL)
                    if triage_section_match:
                        if '[critical]' in triage_section_match.group(1):
                            omega_data['severity'] = 'critical'
                        elif '[high]' in triage_section_match.group(1):
                            omega_data['severity'] = 'high'

                    res_match = re.search(r'RESOLUTION STRATEGY:\s*\n(.*?)(?:\n\s*â””â”€|\Z)', search_area, re.DOTALL)
                    if res_match:
                        strategy = res_match.group(1).strip()
                        cleaned_strategy = "\n".join(line.strip().lstrip('â”‚').lstrip() for line in strategy.split('\n'))
                        omega_data['resolution_strategy'] = cleaned_strategy.strip()

            if omega_data['name'] != 'N/A':
                omegas.append(omega_data)


    # Deduplicate based on the unique name of the Omega
    unique_omegas = []
    seen = set()
    for o in omegas:
        identifier = o['name']
        if identifier not in seen:
            unique_omegas.append(o)
            seen.add(identifier)

    return unique_omegas

def generate_markdown_report(omega_data, output_path):
    """
    Generates a Markdown report from the list of Omega data.
    """
    sorted_omegas = sorted(omega_data, key=lambda x: (x['severity'] != 'critical', x['name']))

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# Omega Epistemological Gap Report\n\n")
        f.write(f"**Total Unique Omegas Found:** {len(sorted_omegas)}\n\n")
        f.write("This report lists all unique Omega variables generated during the analysis. Each Omega represents a critical question, a perspectival gap, or a contradiction that requires further investigation.\n\n")
        f.write("---\n\n")

        for i, o in enumerate(sorted_omegas, 1):
            f.write(f"### {i}. Omega: `{o['name']}`\n\n")
            f.write(f"*   **Severity:** `{o['severity']}`\n")
            f.write(f"*   **Associated Constraint:** `{o['associated_constraint']}`\n")
            f.write(f"*   **Source Gap:** `{o['source_gap']}`\n")
            f.write(f"*   **Question:** {o['question']}\n")
            f.write(f"*   **Suggested Resolution Strategy:**\n")
            f.write(f"    ```\n{o['resolution_strategy']}\n    ```\n\n")
            f.write("---\n\n")

def main():
    """
    Main function to run the reporter.
    """
    script_dir = Path(__file__).parent
    log_file = script_dir / '../outputs/output.txt'
    report_file = script_dir / '../outputs/omega_report.md'
    
    print("Parsing log file to find Omegas...")
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            log_content = f.read()
    except FileNotFoundError:
        print(f"Error: Log file not found at {log_file}", file=sys.stderr)
        sys.exit(1)
        
    omega_data = parse_log_content(log_content)
    
    if omega_data:
        print(f"Found {len(omega_data)} unique Omegas.")
        print(f"Generating report at {report_file}...")
        generate_markdown_report(omega_data, report_file)
        print("Report generated successfully.")
    else:
        print("No Omegas found in the log file.")

if __name__ == '__main__':
    main()
import os
import re

# The Omegas to Stress Test
OMEGA_TARGETS = {
    "gale_shapley.pl": {"E": "0.8", "S": "0.8"},
    "planetary_boundaries.pl": {"E": "0.8", "S": "0.8"}
}

def apply_stress_test(directory):
    resolved = 0
    for filename, priors in OMEGA_TARGETS.items():
        path = os.path.join(directory, filename)
        if os.path.exists(path):
            with open(path, 'r') as f:
                content = f.read()

            # Update base_extractiveness
            new_content = re.sub(
                r'domain_priors:base_extractiveness\(([^,]+),\s*[\d\.]+\)',
                f'domain_priors:base_extractiveness(\\1, {priors["E"]})',
                content
            )
            # Update suppression_score
            new_content = re.sub(
                r'domain_priors:suppression_score\(([^,]+),\s*[\d\.]+\)',
                f'domain_priors:suppression_score(\\1, {priors["S"]})',
                new_content
            )

            if content != new_content:
                with open(path, 'w') as f:
                    f.write(new_content)
                print(f"âš¡ Stress Test Applied: {filename} (E={priors['E']}, S={priors['S']})")
                resolved += 1
    return resolved

if __name__ == "__main__":
    testset_dir = "../prolog/testsets"
    if os.path.exists(testset_dir):
        count = apply_stress_test(testset_dir)
        print(f"\n--- STRESS TEST COMPLETE: {count} Domains Re-calibrated ---")
import re
import json
import os

def parse_validation_log(log_path, output_json):
    with open(log_path, 'r') as f:
        content = f.read()

    # Split the log by domain entries
    entries = re.split(r'\[\d+\] DOMAIN:', content)
    structured_data = {}

    for entry in entries[1:]:
        # Extract Domain Name and Path
        header_match = re.search(r'^\s*(\S+)\s+\(([^)]+)\)', entry)
        if not header_match: continue

        domain_id = header_match.group(1)
        file_path = header_match.group(2)

        # Initialize record
        record = {
            "path": file_path,
            "repaired_vectors": [],
            "ontological_fraud": [],
            "omegas": [],
            "perspectival_gaps": False,
            "kappas": {}
        }

        # 1. Capture Repaired Vectors (Manual or Imputed)
        repaired = re.findall(r'\[FIXED\] Imputed ([\d\.]+) for ([^ ]+) at T=([\d\.]+)', entry)
        for val, metric, time in repaired:
            record["repaired_vectors"].append({"metric": metric, "t": time, "val": val})

        # 2. Capture Ontological Fraud (Schema/Validation Errors)
        fraud = re.findall(r'ERROR: Invalid ([^\n]+)', entry)
        record["ontological_fraud"].extend(fraud)

        # 3. Capture Omegas (Irreducible Uncertainties)
        omegas = re.findall(r'omega_variable\(\s*([^,]+),\s*"([^"]+)"', entry)
        for o_id, description in omegas:
            record["omegas"].append({"id": o_id, "desc": description})

        # 4. Check for Perspectival Gaps (Type1 \= Type2 in tests)
        if "perspectival_gap" in entry and "Passed" in entry:
            record["perspectival_gap"] = True

        # 5. Capture Kappas/Confidence (if printed in your feedback generator)
        # Note: Adjust regex based on your report_generator:generate_llm_feedback format
        kappas = re.findall(r'confidence_without_resolution\(([^)]+)\)', entry)
        if kappas:
            record["kappas"]["default"] = kappas[0]

        structured_data[domain_id] = record

    with open(output_json, 'w') as f:
        json.dump(structured_data, f, indent=4)

    return structured_data

if __name__ == "__main__":
    parse_validation_log("../outputs/output.txt", "../outputs/structured_analysis.json")
    print("âœ“ Analysis complete: outputs/structured_analysis.json generated.")
#!/usr/bin/env python3
"""
Pattern Miner - Report 3: Structural Pattern Mining

Finds structural twins and patterns that suggest new constraint categories.
Identifies candidates for: Tangled Rope, Piton, Scaffold, Wings
"""

import json
from collections import defaultdict, Counter
from pathlib import Path

class PatternMiner:
    """Mines structural patterns to identify candidate categories"""

    def __init__(self, corpus_data_path):
        with open(corpus_data_path, 'r') as f:
            data = json.load(f)

        self.constraints = data['constraints']
        self.summary = data['summary']

    def analyze(self):
        """Run complete pattern mining analysis"""
        results = {}

        results['structural_twins'] = self.find_structural_twins()
        results['candidate_categories'] = self.identify_candidate_categories()
        results['hybrid_patterns'] = self.find_hybrid_patterns()
        results['transition_markers'] = self.find_transition_markers()
        results['recommendations'] = self.generate_recommendations(results)

        return results

    def structural_signature(self, constraint):
        """Calculate structural signature for grouping"""
        metrics = constraint.get('metrics', {})

        extractiveness = metrics.get('extractiveness')
        suppression = metrics.get('suppression')
        emerges = metrics.get('emerges_naturally')
        enforced = metrics.get('requires_enforcement')

        # Round metrics to 1 decimal for grouping
        if extractiveness is not None:
            extractiveness = round(extractiveness, 1)
        if suppression is not None:
            suppression = round(suppression, 1)

        return (extractiveness, suppression, emerges, enforced)

    def find_structural_twins(self):
        """Find constraints with same structure but different claimed types"""
        by_signature = defaultdict(list)

        for cid, constraint in self.constraints.items():
            signature = self.structural_signature(constraint)
            domain = constraint.get('domain') or 'unknown'
            by_signature[signature].append({
                'id': cid,
                'claimed_type': constraint.get('claimed_type'),
                'domain': domain,
                'extractiveness': constraint.get('metrics', {}).get('extractiveness'),
                'suppression': constraint.get('metrics', {}).get('suppression'),
                'emerges_naturally': constraint.get('metrics', {}).get('emerges_naturally'),
                'requires_enforcement': constraint.get('metrics', {}).get('requires_enforcement')
            })

        # Find groups with multiple types
        twins = []
        for signature, group in by_signature.items():
            if len(group) > 1:
                types = [c['claimed_type'] for c in group if c['claimed_type']]
                unique_types = set(types)

                if len(unique_types) > 1:
                    twins.append({
                        'signature': signature,
                        'count': len(group),
                        'types_present': list(unique_types),
                        'domains': list(set(c['domain'] for c in group)),
                        'examples': [c['id'] for c in group[:5]]
                    })

        # Sort by count (most common patterns first)
        twins.sort(key=lambda x: x['count'], reverse=True)

        return twins

    def identify_candidate_categories(self):
        """Identify constraints matching candidate category patterns"""
        candidates = {
            'tangled_rope': [],
            'piton': [],
            'scaffold': [],
            'wings': []
        }

        for cid, constraint in self.constraints.items():
            metrics = constraint.get('metrics', {})
            extractiveness = metrics.get('extractiveness')
            suppression = metrics.get('suppression')
            emerges = metrics.get('emerges_naturally')
            enforced = metrics.get('requires_enforcement')

            # Skip if missing critical metrics
            if extractiveness is None or suppression is None:
                continue

            domain = constraint.get('domain') or 'unknown'
            entry = {
                'constraint_id': cid,
                'claimed_type': constraint.get('claimed_type'),
                'domain': domain,
                'extractiveness': extractiveness,
                'suppression': suppression,
                'emerges_naturally': emerges,
                'requires_enforcement': enforced
            }

            # Tangled Rope: High extraction + High suppression + Enforced
            # (Mix of snare and rope characteristics)
            if (extractiveness >= 0.6 and
                suppression >= 0.6 and
                enforced):
                candidates['tangled_rope'].append(entry)

            # Piton: High suppression + Enforced but still claimed as mountain
            # (False mountain that's obviously constructed)
            if (suppression >= 0.7 and
                enforced and
                constraint.get('claimed_type') == 'mountain'):
                candidates['piton'].append(entry)

            # Scaffold: Medium everything, temporary transition markers
            # (Not extreme on any dimension)
            if (0.3 <= extractiveness <= 0.6 and
                0.3 <= suppression <= 0.6):
                candidates['scaffold'].append(entry)

            # Wings: Low extraction + Low suppression + Emerges naturally
            # (Enabling constraints, opposite of snare)
            if (extractiveness <= 0.3 and
                suppression <= 0.3 and
                emerges):
                candidates['wings'].append(entry)

        # Add counts and statistics
        category_stats = {}
        for category, items in candidates.items():
            if items:
                type_distribution = Counter(item['claimed_type'] for item in items if item['claimed_type'])
                domain_distribution = Counter(item['domain'] for item in items)

                category_stats[category] = {
                    'count': len(items),
                    'examples': items[:10],
                    'type_distribution': dict(type_distribution),
                    'domain_distribution': dict(domain_distribution)
                }
            else:
                category_stats[category] = {
                    'count': 0,
                    'examples': [],
                    'type_distribution': {},
                    'domain_distribution': {}
                }

        return category_stats

    def find_hybrid_patterns(self):
        """Find constraints exhibiting hybrid characteristics"""
        hybrids = []

        for cid, constraint in self.constraints.items():
            metrics = constraint.get('metrics', {})
            extractiveness = metrics.get('extractiveness')
            suppression = metrics.get('suppression')

            if extractiveness is None or suppression is None:
                continue

            # Hybrid: Both extractiveness and suppression are significant
            # (Not clearly snare, rope, or mountain)
            if extractiveness >= 0.5 and suppression >= 0.5:
                analysis = constraint.get('analysis', {})
                variance = analysis.get('variance_ratio')
                domain = constraint.get('domain') or 'unknown'

                hybrids.append({
                    'constraint_id': cid,
                    'claimed_type': constraint.get('claimed_type'),
                    'domain': domain,
                    'extractiveness': extractiveness,
                    'suppression': suppression,
                    'variance_ratio': variance,
                    'is_constructed': analysis.get('is_constructed')
                })

        # Sort by total intensity (extractiveness + suppression)
        hybrids.sort(key=lambda x: (x['extractiveness'] + x['suppression']), reverse=True)

        return hybrids

    def find_transition_markers(self):
        """Find constraints that might represent transitions between states"""
        transitions = []

        for cid, constraint in self.constraints.items():
            metrics = constraint.get('metrics', {})
            extractiveness = metrics.get('extractiveness')
            suppression = metrics.get('suppression')
            resistance = metrics.get('resistance')

            if extractiveness is None or suppression is None:
                continue

            # Transition marker: Medium values suggest in-between state
            # Check if values are in the middle range
            mid_range_count = 0
            if 0.3 <= extractiveness <= 0.7:
                mid_range_count += 1
            if 0.3 <= suppression <= 0.7:
                mid_range_count += 1
            if resistance is not None and 0.3 <= resistance <= 0.7:
                mid_range_count += 1

            # If 2+ metrics are mid-range, could be transition
            if mid_range_count >= 2:
                domain = constraint.get('domain') or 'unknown'
                transitions.append({
                    'constraint_id': cid,
                    'claimed_type': constraint.get('claimed_type'),
                    'domain': domain,
                    'extractiveness': extractiveness,
                    'suppression': suppression,
                    'resistance': resistance,
                    'mid_range_count': mid_range_count
                })

        # Sort by mid-range count
        transitions.sort(key=lambda x: x['mid_range_count'], reverse=True)

        return transitions

    def generate_recommendations(self, results):
        """Generate recommendations based on pattern findings"""
        recommendations = []

        # Check structural twins
        twins = results['structural_twins']
        if twins:
            high_count_twins = [t for t in twins if t['count'] >= 5]
            if high_count_twins:
                recommendations.append({
                    'priority': 'HIGH',
                    'finding': f"Found {len(high_count_twins)} structural signatures shared by 5+ constraints",
                    'action': "Investigate if these represent distinct categories beyond current framework",
                    'examples': [t['signature'] for t in high_count_twins[:3]]
                })

        # Check candidate categories
        candidates = results['candidate_categories']

        for category, stats in candidates.items():
            if stats['count'] >= 10:
                recommendations.append({
                    'priority': 'MEDIUM',
                    'finding': f"Found {stats['count']} constraints matching '{category}' pattern",
                    'action': f"Consider formalizing '{category}' as new category",
                    'details': f"Type distribution: {stats['type_distribution']}"
                })

        # Check hybrids
        hybrids = results['hybrid_patterns']
        if len(hybrids) >= 20:
            recommendations.append({
                'priority': 'HIGH',
                'finding': f"Found {len(hybrids)} hybrid constraints (high extraction + high suppression)",
                'action': "Strong evidence for 'Tangled Rope' category",
                'details': f"{len(hybrids)} constraints don't fit cleanly into mountain/rope/snare"
            })

        # Check transitions
        transitions = results['transition_markers']
        if len(transitions) >= 15:
            recommendations.append({
                'priority': 'MEDIUM',
                'finding': f"Found {len(transitions)} constraints with mid-range metrics",
                'action': "Consider 'Scaffold' category for temporary/transitional constraints",
                'details': "These constraints show characteristics of multiple types"
            })

        # Sort by priority
        priority_order = {'HIGH': 0, 'MEDIUM': 1, 'LOW': 2}
        recommendations.sort(key=lambda x: priority_order[x['priority']])

        return recommendations

    def generate_report(self, output_path):
        """Generate markdown report"""
        results = self.analyze()

        with open(output_path, 'w') as f:
            f.write("# Structural Pattern Mining\n\n")

            # Executive Summary
            f.write("## Executive Summary\n\n")

            recommendations = results['recommendations']
            if recommendations:
                f.write("### Key Findings\n\n")
                for i, rec in enumerate(recommendations[:5], 1):
                    f.write(f"{i}. **[{rec['priority']}]** {rec['finding']}\n")
                    f.write(f"   - Action: {rec['action']}\n")
                    if 'details' in rec:
                        f.write(f"   - Details: {rec['details']}\n")
                    f.write("\n")
            else:
                f.write("No significant patterns detected requiring new categories.\n\n")

            # Structural Twins
            f.write("## Structural Twins\n\n")
            f.write("Constraints with identical structural signatures but different claimed types.\n\n")

            twins = results['structural_twins']
            if twins:
                f.write(f"**Total twin groups found:** {len(twins)}\n\n")

                f.write("| Signature | Count | Types Present | Domains | Examples |\n")
                f.write("|-----------|-------|---------------|---------|----------|\n")

                for twin in twins[:20]:
                    sig_str = str(twin['signature'])[:30]
                    types_str = ', '.join(twin['types_present'])
                    # Filter out None domains before joining
                    valid_domains = [d for d in twin['domains'][:3] if d is not None]
                    domains_str = ', '.join(valid_domains) if valid_domains else 'unknown'
                    examples_str = ', '.join(twin['examples'][:2])

                    f.write(f"| {sig_str:30s} | {twin['count']:5d} | {types_str:20s} | {domains_str:15s} | {examples_str:30s} |\n")

                f.write("\n")
            else:
                f.write("No structural twins detected.\n\n")

            # Candidate Categories
            f.write("## Candidate Category Analysis\n\n")

            candidates = results['candidate_categories']

            for category in ['tangled_rope', 'piton', 'scaffold', 'wings']:
                stats = candidates[category]
                f.write(f"### {category.replace('_', ' ').title()}\n\n")

                if category == 'tangled_rope':
                    f.write("**Pattern:** High extraction + High suppression + Requires enforcement\n")
                    f.write("**Interpretation:** Mix of snare and rope characteristics\n\n")
                elif category == 'piton':
                    f.write("**Pattern:** High suppression + Enforced + Claimed as mountain\n")
                    f.write("**Interpretation:** False mountains that are obviously constructed\n\n")
                elif category == 'scaffold':
                    f.write("**Pattern:** Medium extractiveness + Medium suppression\n")
                    f.write("**Interpretation:** Temporary transition mechanisms\n\n")
                elif category == 'wings':
                    f.write("**Pattern:** Low extraction + Low suppression + Emerges naturally\n")
                    f.write("**Interpretation:** Enabling constraints, opposite of snare\n\n")

                if stats['count'] > 0:
                    f.write(f"**Constraints matching pattern:** {stats['count']}\n\n")

                    if stats['type_distribution']:
                        f.write("**Current type distribution:**\n")
                        for ctype, count in sorted(stats['type_distribution'].items(), key=lambda x: x[1], reverse=True):
                            f.write(f"- {ctype}: {count}\n")
                        f.write("\n")

                    if stats['examples']:
                        f.write("**Examples:**\n\n")
                        f.write("| Constraint ID | Claimed Type | Extractiveness | Suppression | Domain |\n")
                        f.write("|---------------|--------------|----------------|-------------|--------|\n")

                        for example in stats['examples'][:10]:
                            claimed = example['claimed_type'] or 'N/A'
                            f.write(f"| {example['constraint_id']:30s} | {claimed:15s} | {example['extractiveness']:14.2f} | {example['suppression']:11.2f} | {example['domain']:10s} |\n")

                        f.write("\n")
                else:
                    f.write("No constraints match this pattern.\n\n")

            # Hybrid Patterns
            f.write("## Hybrid Patterns\n\n")
            f.write("Constraints with both high extraction and high suppression.\n\n")

            hybrids = results['hybrid_patterns']
            if hybrids:
                f.write(f"**Total hybrids found:** {len(hybrids)}\n\n")

                f.write("| Constraint ID | Claimed Type | Extraction | Suppression | Total | Domain |\n")
                f.write("|---------------|--------------|------------|-------------|-------|--------|\n")

                for hybrid in hybrids[:20]:
                    total = hybrid['extractiveness'] + hybrid['suppression']
                    claimed = hybrid['claimed_type'] or 'N/A'
                    f.write(f"| {hybrid['constraint_id']:30s} | {claimed:15s} | {hybrid['extractiveness']:10.2f} | {hybrid['suppression']:11.2f} | {total:5.2f} | {hybrid['domain']:10s} |\n")

                f.write("\n")
                f.write("**Note:** High 'Total' values indicate constraints that don't fit cleanly into single category.\n\n")
            else:
                f.write("No significant hybrid patterns detected.\n\n")

            # Transition Markers
            f.write("## Transition Markers\n\n")
            f.write("Constraints with mid-range metrics suggesting transitional states.\n\n")

            transitions = results['transition_markers']
            if transitions:
                f.write(f"**Total transition markers found:** {len(transitions)}\n\n")

                f.write("| Constraint ID | Claimed Type | Extraction | Suppression | Resistance | Domain |\n")
                f.write("|---------------|--------------|------------|-------------|------------|--------|\n")

                for trans in transitions[:20]:
                    resistance_str = f"{trans['resistance']:.2f}" if trans['resistance'] is not None else 'N/A'
                    claimed = trans['claimed_type'] or 'N/A'
                    f.write(f"| {trans['constraint_id']:30s} | {claimed:15s} | {trans['extractiveness']:10.2f} | {trans['suppression']:11.2f} | {resistance_str:10s} | {trans['domain']:10s} |\n")

                f.write("\n")
            else:
                f.write("No significant transition markers detected.\n\n")

            # Recommendations
            f.write("## Recommendations\n\n")

            if recommendations:
                for i, rec in enumerate(recommendations, 1):
                    f.write(f"### {i}. {rec['finding']}\n\n")
                    f.write(f"**Priority:** {rec['priority']}\n\n")
                    f.write(f"**Recommended Action:** {rec['action']}\n\n")
                    if 'details' in rec:
                        f.write(f"**Details:** {rec['details']}\n\n")
                    if 'examples' in rec:
                        f.write("**Example signatures:**\n")
                        for ex in rec['examples']:
                            f.write(f"- {ex}\n")
                        f.write("\n")
            else:
                f.write("Current mountain/rope/snare taxonomy appears sufficient.\n")
                f.write("No strong evidence for new categories at this time.\n\n")

        print(f"Report generated: {output_path}")

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Generate structural pattern mining report (Report 3)'
    )
    parser.add_argument(
        '--corpus-data',
        default='../outputs/corpus_data.json',
        help='Path to corpus_data.json'
    )
    parser.add_argument(
        '--output',
        default='../outputs/pattern_mining.md',
        help='Output markdown file'
    )

    args = parser.parse_args()

    miner = PatternMiner(args.corpus_data)
    miner.generate_report(args.output)

if __name__ == '__main__':
    main()
import re
import sys
from pathlib import Path

def parse_log_content(content):
    """
    Parses the log content to find and extract Piton details.
    """
    pitons = []
    
    # Split the entire log into chunks, each starting with a new scenario load.
    scenario_chunks = re.split(r'(?=\[SCENARIO MANAGER\] Clearing Knowledge Base\.\.\.)', content)

    for chunk in scenario_chunks:
        if not chunk.strip():
            continue

        # Get the name of the constraint being tested in this chunk
        name_match = re.search(r'Loading: \.\./prolog/testsets/(.+?)\.pl', chunk)
        if not name_match:
            continue
        constraint_name = name_match.group(1)

        # Check if the claimed type is 'piton' within the INDEXICAL AUDIT section
        claimed_type_match = re.search(r'Constraint: ' + re.escape(constraint_name) + r'\s*\n\s*Claimed Type: (\w+)', chunk)
        if not claimed_type_match or claimed_type_match.group(1) != 'piton':
            continue # Not a claimed piton, so skip for this report

        # Found a claimed piton, now extract all its details
        piton_data = {
            'name': constraint_name,
            'claimed_type': claimed_type_match.group(1),
            'powerless_view': 'N/A',
            'institutional_view': 'N/A',
            'analytical_view': 'N/A',
            'structural_signature': 'N/A',
            'related_gap_alert': 'N/A',
            'omega_question': 'N/A',
            'severity': 'N/A',
            'resolution_strategy': ''
        }

        # Extract perspective classifications from the INDEXICAL AUDIT section
        perspectives_section = re.search(r'^\[CONSTRAINT INVENTORY: INDEXICAL AUDIT\]\s*\n.*?Perspectives:(.*?)(?=\n\n|\n\[CROSS-DOMAIN ISOMORPHISM)', chunk, re.DOTALL)
        if perspectives_section:
            for line in perspectives_section.group(1).split('\n'):
                if 'Individual (Powerless):' in line:
                    piton_data['powerless_view'] = re.search(r': (\w+)', line).group(1)
                elif 'Institutional (Manager):' in line:
                    piton_data['institutional_view'] = re.search(r': (\w+)', line).group(1)
                elif 'Analytical:' in line: 
                    piton_data['analytical_view'] = re.search(r': (\w+)', line).group(1)

        # Extract Structural Signature Analysis
        signature_match = re.search(r'^\[STRUCTURAL SIGNATURE ANALYSIS\]\s*\n.*?â†’ (.+)', chunk, re.DOTALL)
        if signature_match:
            piton_data['structural_signature'] = signature_match.group(1).strip()
        
        # Extract related gap/alert (if any)
        # Look for this in the PERSPECTIVAL GAP ANALYSIS section specifically
        gap_analysis_section = re.search(r'^\[PERSPECTIVAL GAP ANALYSIS\](.*?)(?=\n\[OMEGA GENERATION|\n\[OMEGA TRIAGE)', chunk, re.DOTALL)
        if gap_analysis_section:
            gap_alert_match = re.search(r'(! (?:ALERT|GAP): .+)', gap_analysis_section.group(1))
            if gap_alert_match:
                piton_data['related_gap_alert'] = gap_alert_match.group(1).strip()

        # Extract Omega from OMEGA GENERATION section
        omega_section = re.search(r'^\[OMEGA GENERATION FROM PERSPECTIVAL GAPS: ' + re.escape(constraint_name) + r'\](.*?)(?=\n\[OMEGA TRIAGE)', chunk, re.DOTALL)
        if omega_section:
            omega_match = re.search(r'Î©: (.+)', omega_section.group(1))
            if omega_match:
                piton_data['omega_question'] = omega_match.group(1).strip()

        # Extract Severity from the Triage section
        triage_section = re.search(r'^\[OMEGA TRIAGE & PRIORITIZATION\](.*?)(?=\n\n|\n\[OMEGA RESOLUTION)', chunk, re.DOTALL)
        if triage_section:
            if '[critical]' in triage_section.group(1):
                piton_data['severity'] = 'critical'
            elif '[high]' in triage_section.group(1): # Prioritize critical over high
                piton_data['severity'] = 'high'
        
        # Extract Resolution Strategy
        res_match = re.search(r'RESOLUTION STRATEGY:\s*\n(.*?)(?:\n\s*â””â”€|\Z)', chunk, re.DOTALL)
        if res_match:
            strategy = res_match.group(1).strip()
            cleaned_strategy = "\n".join(line.strip().lstrip('â”‚').lstrip() for line in strategy.split('\n'))
            piton_data['resolution_strategy'] = cleaned_strategy

        # Add to list if we have the core info and it's a genuine piton
        if piton_data['claimed_type'] == 'piton':
            pitons.append(piton_data)

    # Deduplicate based on constraint name and omega question, if multiple omegas were generated for the same constraint
    unique_pitons = []
    seen = set()
    for n in pitons:
        identifier = (n['name'], n['omega_question']) 
        if identifier not in seen:
            unique_pitons.append(n)
            seen.add(identifier)

    return unique_pitons

def generate_markdown_report(pitons_data, output_path):
    """
    Generates a Markdown report from the list of Piton data.
    """
    # Sort by severity (critical first), then by name
    sorted_pitons = sorted(pitons_data, key=lambda x: (x['severity'] != 'critical', x['name']))

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# Piton Diagnostic Report\n\n")
        f.write(f"**Total Unique Pitons Found:** {len(sorted_pitons)}\n\n")
        f.write("---\n\n")

        for i, n in enumerate(sorted_pitons, 1):
            f.write(f"### {i}. Piton: `{n['name']}`\n\n")
            f.write(f"*   **Claimed Type:** `{n['claimed_type']}`\n")
            f.write(f"*   **Severity:** `{n['severity']}`\n")
            
            f.write(f"*   **Perspectival Breakdown:**\n")
            if n['powerless_view'] != 'N/A':
                f.write(f"    *   **Individual (Powerless) View:** `{n['powerless_view']}`\n")
            if n['institutional_view'] != 'N/A':
                f.write(f"    *   **Institutional (Manager) View:** `{n['institutional_view']}`\n")
            if n['analytical_view'] != 'N/A': 
                f.write(f"    *   **Analytical View:** `{n['analytical_view']}`\n")
            
            f.write(f"*   **Structural Signature Analysis:** {n['structural_signature']}\n")
            
            if n['related_gap_alert'] != 'N/A':
                f.write(f"*   **Related Gap/Alert:** {n['related_gap_alert']}\n")
            
            f.write(f"*   **Generated Omega:** {n['omega_question']}\n")
            f.write(f"*   **Suggested Resolution Strategy:**\n")
            f.write(f"    ```\n{n['resolution_strategy']}\n    ```\n\n")
            f.write("---\n\n")

def main():
    """
    Main function to run the reporter.
    """
    script_dir = Path(__file__).parent
    log_file = script_dir / '../outputs/output.txt'
    report_file = script_dir / '../outputs/piton_report.md'
    
    print("Parsing log file to find Pitons...")
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            log_content = f.read()
    except FileNotFoundError:
        print(f"Error: Log file not found at {log_file}", file=sys.stderr)
        sys.exit(1)
        
    pitons_data = parse_log_content(log_content)
    
    if pitons_data:
        print(f"Found {len(pitons_data)} unique Pitons.")
        print(f"Generating report at {report_file}...")
        generate_markdown_report(pitons_data, report_file)
        print("Report generated successfully.")
    else:
        print("No Pitons found in the log file.")

if __name__ == '__main__':
    main()
#!/usr/bin/env python3
"""
Prolog File Cleaner - Fixes common AI-generated artifacts

Common issues from Gemini and other LLMs:
1. [cite_start]% comments - should be %[cite_start]
2. ## markdown headers in Prolog files
3. Broken comment blocks
4. Extra whitespace
"""

import os
import re
import sys
from pathlib import Path

class PrologCleaner:
    def __init__(self):
        self.fixes_applied = 0
        self.files_cleaned = 0
        self.errors_found = []

    def clean_line(self, line):
        """Clean a single line of Prolog code"""
        original = line

        # Fix 1: [cite_start]% -> %[cite_start]
        # Match any citation marker followed by %
        if re.match(r'^\s*\[cite', line):
            line = re.sub(r'^(\s*)\[cite([^\]]*)\]%', r'\1%[cite\2]', line)
            if line != original:
                self.fixes_applied += 1

        # Fix 2: Remove ## markdown headers
        # Turn ## Header into % Header (proper Prolog comment)
        if line.strip().startswith('##'):
            line = re.sub(r'^(\s*)##\s*', r'\1% ', line)
            if line != original:
                self.fixes_applied += 1

        # Fix 3: Remove # at start of comment blocks (but not inside strings)
        # Match # at beginning of line followed by space (not inside quotes)
        if re.match(r'^\s*#\s+', line) and '"' not in line:
            line = re.sub(r'^(\s*)#\s+', r'\1% ', line)
            if line != original:
                self.fixes_applied += 1

        # Fix 4: Normalize comment markers (ensure space after %)
        if re.match(r'^\s*%[^\s%]', line):
            line = re.sub(r'^(\s*)%([^\s%])', r'\1% \2', line)
            if line != original:
                self.fixes_applied += 1

        return line

    def clean_file(self, filepath):
        """Clean a single Prolog file"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except Exception as e:
            self.errors_found.append(f"Error reading {filepath}: {e}")
            return False

        cleaned_lines = []
        file_modified = False

        for i, line in enumerate(lines, 1):
            cleaned = self.clean_line(line)
            if cleaned != line:
                file_modified = True
            cleaned_lines.append(cleaned)

        if file_modified:
            try:
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.writelines(cleaned_lines)
                self.files_cleaned += 1
                return True
            except Exception as e:
                self.errors_found.append(f"Error writing {filepath}: {e}")
                return False

        return False

    def scan_for_issues(self, filepath):
        """Scan file for potential issues without fixing"""
        issues = []

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except Exception as e:
            return [f"Cannot read file: {e}"]

        for i, line in enumerate(lines, 1):
            # Check for citation artifacts
            if '[cite' in line and line.strip().startswith('[cite'):
                issues.append(f"Line {i}: Citation marker at start of line")

            # Check for markdown headers
            if line.strip().startswith('##'):
                issues.append(f"Line {i}: Markdown header (##)")

            # Check for unescaped # in comments
            if re.match(r'^\s*#\s+', line) and '"' not in line:
                issues.append(f"Line {i}: Hash comment instead of %")

            # Check for syntax errors (basic)
            if ':-' in line and not line.strip().endswith('.') and not line.strip().endswith(','):
                if i < len(lines) and not lines[i].strip().startswith('%'):
                    issues.append(f"Line {i}: Possible missing period or comma")

        return issues

    def clean_directory(self, directory, dry_run=False):
        """Clean all .pl files in directory"""
        directory = Path(directory)

        if not directory.exists():
            print(f"Error: Directory {directory} does not exist")
            return

        pl_files = list(directory.glob('*.pl'))

        if not pl_files:
            print(f"No .pl files found in {directory}")
            return

        print(f"{'[DRY RUN] ' if dry_run else ''}Scanning {len(pl_files)} Prolog files...")

        issues_by_file = {}

        for filepath in sorted(pl_files):
            issues = self.scan_for_issues(filepath)

            if issues:
                issues_by_file[filepath.name] = issues

            if not dry_run and issues:
                self.clean_file(filepath)

        # Report results
        print(f"\n{'='*70}")
        print(f"PROLOG CLEANER REPORT")
        print(f"{'='*70}\n")

        if dry_run:
            if issues_by_file:
                print(f"Found issues in {len(issues_by_file)} file(s):\n")
                for filename, issues in sorted(issues_by_file.items()):
                    print(f"  {filename}:")
                    for issue in issues:
                        print(f"    - {issue}")
                    print()
                print(f"Run without --dry-run to automatically fix these issues.")
            else:
                print("âœ“ No issues found in any files.")
        else:
            if self.files_cleaned > 0:
                print(f"âœ“ Cleaned {self.files_cleaned} file(s)")
                print(f"âœ“ Applied {self.fixes_applied} fix(es)")
            else:
                print("âœ“ No files needed cleaning")

            if self.errors_found:
                print(f"\nâš  Errors encountered:")
                for error in self.errors_found:
                    print(f"  - {error}")

        print(f"\n{'='*70}\n")

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Clean Prolog files by fixing common AI-generated artifacts'
    )
    parser.add_argument(
        'directory',
        nargs='?',
        default='../prolog/testsets/',
        help='Directory containing .pl files (default: ../prolog/testsets/)'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Scan for issues without fixing them'
    )
    parser.add_argument(
        '--file',
        help='Clean a specific file instead of directory'
    )

    args = parser.parse_args()

    cleaner = PrologCleaner()

    if args.file:
        # Clean single file
        filepath = Path(args.file)
        if not filepath.exists():
            print(f"Error: File {filepath} does not exist")
            sys.exit(1)

        issues = cleaner.scan_for_issues(filepath)

        if issues:
            print(f"Issues found in {filepath.name}:")
            for issue in issues:
                print(f"  - {issue}")

            if not args.dry_run:
                print(f"\nCleaning {filepath.name}...")
                if cleaner.clean_file(filepath):
                    print(f"âœ“ File cleaned successfully")
                else:
                    print(f"âœ— File was already clean or errors occurred")
        else:
            print(f"âœ“ No issues found in {filepath.name}")
    else:
        # Clean directory
        cleaner.clean_directory(args.directory, dry_run=args.dry_run)

if __name__ == '__main__':
    main()
# Usage:
####################################################
# 1. Place the Python script in your root directory.
# 2. Run python build_suite.py.
# 3. Launch Prolog: swipl validation_suite.pl.
# 4. Run the tests: ?- run_dynamic_suite.

import os
import re

DATASETS_DIR = './datasets/'
OUTPUT_FILE = 'validation_suite.pl'

# Improved Regex: Captures ID regardless of single/double quotes or no quotes
# Matches: interval('ID', ...), interval("ID", ...), or interval(id, ...)
INTERVAL_REGEX = re.compile(r"interval\s*\(\s*['\"]?([a-zA-Z0-9_]+)['\"]?\s*,")

def build_suite():
    test_entries = []

    if not os.path.exists(DATASETS_DIR):
        print(f"Error: Directory {DATASETS_DIR} not found.")
        return

    files = sorted([f for f in os.listdir(DATASETS_DIR) if f.endswith('.pl')])

    for idx, filename in enumerate(files, 1):
        filepath = os.path.join(DATASETS_DIR, filename)
        interval_id = 'unknown_interval'

        try:
            with open(filepath, 'r') as f:
                content = f.read()
                match = INTERVAL_REGEX.search(content)
                if match:
                    interval_id = match.group(1)
                else:
                    print(f"Warning: No interval found in {filename}")
        except Exception as e:
            print(f"Error reading {filename}: {e}")

        label = filename.replace('_data.pl', '').upper()
        # Use single quotes for atoms in Prolog if they aren't standard camelCase
        test_entries.append(f"    test_file('{filepath}', '{interval_id}', '{label}', {idx})")

    with open(OUTPUT_FILE, 'w') as out:
        out.write(":- use_module(scenario_manager).\n")
        out.write(":- dynamic test_passed/1.\n")
        out.write(":- dynamic test_failed/2.\n\n")

        out.write("run_dynamic_suite :-\n")
        out.write("    retractall(test_passed(_)),\n")
        out.write("    retractall(test_failed(_, _)),\n")
        out.write("    writeln('--- STARTING DYNAMIC VALIDATION ---'),\n")
        out.write(',\n'.join(test_entries) + ',\n')
        out.write("    count_and_report.\n\n")

        # FIXED: Updated test_file with if-then-else and cut to prevent infinite retries
        out.write("test_file(Path, ID, Label, N) :-\n")
        out.write("    format('~n[~w] DOMAIN: ~w (~w)~n', [N, Label, Path]),\n")
        out.write("    (   catch(run_scenario(Path, ID), E, (assertz(test_failed(Path, E)), format('[FAIL] Exception: ~w~n', [E]), fail))\n")
        out.write("    ->  assertz(test_passed(Path))\n")
        out.write("    ;   assertz(test_failed(Path, verification_failed))\n")
        out.write("    ),\n")
        out.write("    !.  % CRITICAL: Cut to prevent backtracking/retrying on failure\n\n")

        out.write("count_and_report :-\n")
        out.write("    findall(P, test_passed(P), Ps), length(Ps, PC), findall(F, test_failed(F,_), Fs), length(Fs, FC),\n")
        out.write("    format('~nDONE: ~w Passed, ~w Failed~n', [PC, FC]).\n")

    print(f"âœ“ Generated {OUTPUT_FILE} with {len(test_entries)} tests")
    print(f"âœ“ Includes backtracking fix: if-then-else + cut to prevent infinite retries")

if __name__ == "__main__":
    build_suite()
# python_test_suite.py

import os
import re
import sys

# Construct absolute paths from this script's location
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(SCRIPT_DIR)
DATASETS_DIR = os.path.join(ROOT_DIR, 'prolog', 'testsets')
OUTPUT_FILE = os.path.join(ROOT_DIR, 'prolog', 'validation_suite.pl')

INTERVAL_REGEX = re.compile(r"interval\s*\(\s*'?(\w+)")

def build_suite():
    if not os.path.isdir(DATASETS_DIR):
        print(f"Error: Datasets directory not found at {DATASETS_DIR}")
        sys.exit(1)

    try:
        files = sorted([f for f in os.listdir(DATASETS_DIR) if f.endswith('.pl')])
    except Exception as e:
        print(f"Error listing files in {DATASETS_DIR}: {e}")
        sys.exit(1)

    print(f"Found {len(files)} .pl files in {DATASETS_DIR}")

    test_case_facts = []
    for idx, filename in enumerate(files, 1):
        filepath = f"testsets/{filename}"
        interval_id = 'unknown_interval'
        try:
            full_path = os.path.join(DATASETS_DIR, filename)
            with open(full_path, 'r', encoding='utf-8') as f:
                content = f.read()
                match = INTERVAL_REGEX.search(content)
                if match:
                    interval_id = match.group(1)
        except Exception as e:
            print(f"Warning: Could not read or parse {filename}: {e}")

        label = filename.replace('.pl', '').upper()
        test_case_facts.append(f"test_case('{filepath}', '{interval_id}', '{label}', {idx}).")

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as out:
        out.write(":- module(validation_suite, [run_dynamic_suite/0]).\n")
        out.write(":- use_module(library(prolog_stack)).\n")
        out.write(":- use_module(scenario_manager).\n")
        out.write(":- use_module(data_validation).\n")
        out.write(":- use_module(report_generator).\n\n")
        # Set CWD to the 'prolog' directory to make file paths reliable
        out.write(":- chdir('../prolog').\n\n")
        out.write(":- dynamic test_passed/1, test_failed/3, test_case/4.\n\n")

        out.write("% --- Test Case Definitions ---\n")
        out.write('\n'.join(test_case_facts) + '\n\n')

        out.write("% --- Test Suite Runner ---\n")
        out.write("run_dynamic_suite :-\n")
        out.write("    retractall(test_passed(_)),\n")
        out.write("    retractall(test_failed(_, _, _)),\n")
        out.write("    writeln('--- STARTING DYNAMIC VALIDATION ---'),\n")
        out.write("    forall(test_case(Path, ID, Label, N), run_single_test(Path, ID, Label, N)),\n")
        out.write("    count_and_report,\n")
        out.write("    % Call validate_all directly from data_validation module\n")
        out.write("    data_validation:validate_all.\n\n")

        out.write("% --- Single Test Executor ---\n")
        out.write("run_single_test(Path, ID, _Label, N) :-\n")
        out.write("    format('~n[~w] EXECUTING: ~w~n', [N, Path]),\n")
        out.write("    catch_with_backtrace(\n")
        out.write("        ( load_and_run(Path, ID) ->\n")
        out.write("            assertz(test_passed(Path)),\n")
        out.write("            format('[PASS] ~w~n', [Path])\n")
        out.write("        ;   assertz(test_failed(Path, audit_failed, 'load_and_run returned false')),\n")
        out.write("            format('[AUDIT FAIL] ~w~n', [Path])\n")
        out.write("        ),\n")
        out.write("        E,\n")
        out.write("        (   assertz(test_failed(Path, exception, E)),\n")
        out.write("            format('[FAIL] Exception for ~w: ~w~n', [Path, E]),\n")
        out.write("            print_prolog_backtrace(current_output, E)\n")
        out.write("        )\n")
        out.write("    ),\n")
        out.write("    report_generator:generate_llm_feedback(ID).\n\n")

        out.write("% --- Result Counter & Reporter ---\n")
        out.write("count_and_report :-\n")
        out.write("    findall(P, test_passed(P), Ps), length(Ps, PC),\n")
        out.write("    findall(F, test_failed(F,_,_), Fs), length(Fs, FC),\n")
        out.write("    writeln(''),\n")
        out.write("    writeln('=================================================='),\n")
        out.write("    writeln('           TEST SUITE SUMMARY'),\n")
        out.write("    writeln('=================================================='),\n")
        out.write("    format('Passed: ~w~n', [PC]),\n")
        out.write("    format('Failed: ~w~n', [FC]),\n")
        out.write("    (FC > 0 -> report_failures ; true),\n")
        out.write("    writeln('==================================================').\n\n")

        out.write("report_failures :-\n")
        out.write("    writeln('--- FAILED TESTS ---'),\n")
        out.write("    forall(test_failed(Path, Type, Detail),\n")
        out.write("           format('~n  - [~w] ~w~n    Reason: ~w~n', [Type, Path, Detail])).\n\n")

    print(f"âœ“ Generated {OUTPUT_FILE} with {len(test_case_facts)} tests")

if __name__ == "__main__":
    build_suite()
import os
import json

# Configuration
TESTSETS_DIR = "../prolog/testsets/"
GAP_REPORT = "../outputs/gap_report.json"

# Mapping types to their standard agent perspective
PERSPECTIVE_MAP = {
    "rope": "agent_power(institutional)",
    "noose": "agent_power(individual_powerless)",
    "mountain": "agent_power(analytical)"
}

def inject_skeletons():
    # 1. Load the Gap Analysis
    if not os.path.exists(GAP_REPORT):
        print(f"Error: {GAP_REPORT} not found. Run gap_analysis.py first.")
        return

    with open(GAP_REPORT, 'r') as f:
        gaps = json.load(f)

    repaired_count = 0

    # 2. Iterate through domains with gaps
    for domain_id, status in gaps.items():
        if status["is_complete"]:
            continue

        file_path = os.path.join(TESTSETS_DIR, f"{domain_id}.pl")
        if not os.path.exists(file_path):
            print(f"Warning: File for {domain_id} not found at {file_path}")
            continue

        # Identify missing pillars
        missing_pillars = []
        for pillar in ["rope", "noose", "mountain"]:
            if not status[pillar]:
                missing_pillars.append(pillar)

        if not missing_pillars:
            continue

        # 3. Generate Skeleton Code
        skeleton_block = [
            "\n% ==========================================================================",
            f"% [SKELETON REPAIR] Missing Perspectival Pillars Added: {', '.join(missing_pillars).upper()}",
            "% To be calibrated: Define the narrative justification for these perspectives.",
            "% =========================================================================="
        ]

        for pillar in missing_pillars:
            perspective = PERSPECTIVE_MAP[pillar]
            predicate = f"constraint_indexing:constraint_classification({domain_id}, {pillar}, {perspective})."
            skeleton_block.append(predicate)

        # 4. Append to File
        try:
            with open(file_path, 'a') as f:
                f.write("\n" + "\n".join(skeleton_block) + "\n")
            repaired_count += 1
            print(f"âœ“ Injected {len(missing_pillars)} pillars into {domain_id}.pl")
        except Exception as e:
            print(f"âœ— Failed to repair {domain_id}: {e}")

    print(f"\nSummary: {repaired_count} files updated with skeleton perspectives.")

if __name__ == "__main__":
    inject_skeletons()
import json
import os

def apply_ontological_fix(json_path):
    with open(json_path, 'r') as f:
        data = json.load(f)

    # Standardizing the "Orphan" Ontologies
    ontology_map = {
        "algorithmic_determinism": "mountain",
        "biomountain": "mountain",
        "logic_limit": "mountain"
    }

    repaired_count = 0

    for domain, record in data.items():
        current_ontology = record.get("ontology", "").lower()
        if current_ontology in ontology_map:
            record["ontology"] = ontology_map[current_ontology]
            repaired_count += 1
            print(f"âœ“ Fixed: {domain} ({current_ontology} -> mountain)")

    # Save the hardened analysis
    with open(json_path, 'w') as f:
        json.dump(data, f, indent=4)

    print(f"\n--- REPAIR COMPLETE ---")
    print(f"Total Ontological Violations Resolved: {repaired_count}")

if __name__ == "__main__":
    # Ensure this points to your latest structured output
    analysis_file = "../outputs/structured_analysis.json"
    if os.path.exists(analysis_file):
        apply_ontological_fix(analysis_file)
    else:
        print("Error: structured_analysis.json not found.")
import re
import sys
from pathlib import Path

def parse_log_content(content):
    """
    Parses the log content to find and extract Rope details.
    A Rope is defined as a constraint that is claimed as a rope
    and is classified as a rope from all perspectives (no mismatches).
    """
    ropes = []
    
    scenario_chunks = re.split(r'(?=\[SCENARIO MANAGER\] Clearing Knowledge Base...)', content)

    for chunk in scenario_chunks:
        if not chunk.strip():
            continue

        name_match = re.search(r'Loading: \.\./prolog/testsets/(.+?)\.pl', chunk)
        if not name_match:
            continue
        constraint_name = name_match.group(1)

        # Look for the main audit section
        audit_section_match = re.search(r'\[CONSTRAINT INVENTORY: INDEXICAL AUDIT\]\s*\n(.*?)(?=\n\n\[CROSS-DOMAIN ISOMORPHISM|\Z)', chunk, re.DOTALL)
        if not audit_section_match:
            continue
        
        audit_section = audit_section_match.group(1)

        # 1. Claimed Type must be 'rope'
        claimed_type_match = re.search(r'^\s*Claimed Type: (rope)', audit_section, re.MULTILINE)
        if not claimed_type_match:
            continue

        # 2. There must be at least one perspective, and NO mismatches
        # Also ensure no Mismatch in the whole perspectives section
        if '(Mismatch)' in audit_section or 'Perspectives:' not in audit_section:
            continue

        # And explicitly check that all perspectives are 'rope'
        all_perspectives_are_rope = True
        perspectives_lines = re.findall(r'-\s*\[context\(.*?\)\]:\s*(\w+)', audit_section)
        if not perspectives_lines: # No perspectives listed
            all_perspectives_are_rope = False
        else:
            for classification in perspectives_lines:
                if classification != 'rope':
                    all_perspectives_are_rope = False
                    break
        
        if not all_perspectives_are_rope:
            continue
            
        # If we passed all checks, it's a True Rope. Extract details.
        rope_data = {
            'name': constraint_name,
            'claimed_type': 'rope',
            'structural_signature': 'N/A',
            'related_gap_alert': 'N/A',
            'omega_question': 'N/A',
            'resolution_strategy': ''
        }

        # Extract Structural Signature Analysis
        signature_match = re.search(r'â†’\s*(.+)', chunk)
        if signature_match:
            rope_data['structural_signature'] = signature_match.group(1).strip()
        
        # Extract related gap/alert (if any)
        gap_alert_match = re.search(r'(!\s(?:ALERT|GAP):\s*.+)', chunk)
        if gap_alert_match:
            rope_data['related_gap_alert'] = gap_alert_match.group(1).strip()

        # Extract Omega Question specifically
        omega_question_match = re.search(r'Question:\s*(.+)', chunk)
        if omega_question_match:
            rope_data['omega_question'] = omega_question_match.group(1).strip()
        
        # Extract Resolution Strategy
        res_match = re.search(r'RESOLUTION STRATEGY:\s*\n(.*?)(?:\n\s*â””â”€|\n\n### START LLM REFINEMENT MANIFEST|\Z)', chunk, re.DOTALL)
        if res_match:
            strategy = res_match.group(1).strip()
            cleaned_strategy = "\n".join(line.strip().lstrip('â”‚').lstrip() for line in strategy.split('\n'))
            rope_data['resolution_strategy'] = cleaned_strategy.strip()

        ropes.append(rope_data)

    # Deduplicate
    unique_ropes = []
    seen = set()
    for r in ropes:
        identifier = r['name']
        if identifier not in seen:
            unique_ropes.append(r)
            seen.add(identifier)

    return unique_ropes

def generate_markdown_report(rope_data, output_path):
    """
    Generates a Markdown report from the list of Rope data.
    """
    sorted_ropes = sorted(rope_data, key=lambda x: x['name'])

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# Rope Validation Report\n\n")
        f.write(f"**Total Validated:** {len(sorted_ropes)}\n\n")
        f.write("This report lists all constraints that are consistently classified as 'rope' across all tested perspectives, indicating their functional and potentially beneficial nature within the model.\n\n")
        f.write("---\n\n")

        for i, r in enumerate(sorted_ropes, 1):
            f.write(f"### {i}. Rope: `{r['name']}`\n\n")
            f.write(f"*   **Claimed Type:** `{r['claimed_type']}`\n")
            f.write(f"*   **Structural Signature Analysis:** {r['structural_signature']}\n")
            f.write(f"*   **Perspectival Agreement:** Confirmed. All tested perspectives agree on the 'rope' classification.\n")
            
            if r['related_gap_alert'] != 'N/A':
                f.write(f"*   **Related Gap/Alert:** {r['related_gap_alert']}\n")
            if r['omega_question'] != 'N/A':
                f.write(f"*   **Generated Omega:** {r['omega_question']}\n")
            if r['resolution_strategy'] != '':
                f.write(f"*   **Suggested Resolution Strategy:**\n")
                f.write(f"    ```\n{r['resolution_strategy']}\n    ```\n\n")
            else:
                f.write("\n") # Add a newline if no strategy to maintain spacing
            f.write("---\n\n")

def main():
    """
    Main function to run the reporter.
    """
    script_dir = Path(__file__).parent
    log_file = script_dir / '../outputs/output.txt'
    report_file = script_dir / '../outputs/rope_report.md'
    
    print("Parsing log file to find Ropes...")
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            log_content = f.read()
    except FileNotFoundError:
        print(f"Error: Log file not found at {log_file}", file=sys.stderr)
        sys.exit(1)
        
    rope_data = parse_log_content(log_content)
    
    if rope_data:
        print(f"Found {len(rope_data)} validated Ropes.")
        print(f"Generating report at {report_file}...")
        generate_markdown_report(rope_data, report_file)
        print("Report generated successfully.")
    else:
        print("No validated Ropes found in the log file.")

if __name__ == '__main__':
    main()
import json

def find_shadow_nooses(json_path):
    with open(json_path, 'r') as f:
        data = json.load(f)

    print(f"{'DOMAIN':<40} | {'SUPPRESSION':<12} | {'STAKES':<8} | {'STATUS'}")
    print("-" * 75)

    for domain, record in data.items():
        # Look for high suppression and high stakes in the repairs
        repairs = record.get("repaired_vectors", [])
        sup = max([float(r['val']) for r in repairs if 'suppression' in r['metric']] + [0.0])
        stk = max([float(r['val']) for r in repairs if 'stakes' in r['metric']] + [0.0])

        # A "Shadow Noose" is a domain where suppression energy > 0.7
        if sup > 0.7 and stk > 0.7:
            status = "SHADOW NOOSE"
            print(f"{domain:<40} | {sup:<12} | {stk:<8} | {status}")

if __name__ == "__main__":
    find_shadow_nooses("../outputs/structured_analysis.json")
import re
import sys
from pathlib import Path

def parse_log_content(content):
    """
    Parses the log content to find and extract Snare details.
    """
    snares = []
    
    # Split the entire log into chunks, each starting with a new scenario load.
    scenario_chunks = re.split(r'(?=\[SCENARIO MANAGER\] Clearing Knowledge Base\.\.\.)', content)

    for chunk in scenario_chunks:
        if not chunk.strip():
            continue

        # Get the name of the constraint being tested in this chunk
        name_match = re.search(r'Loading: \.\./prolog/testsets/(.+?)\.pl', chunk)
        if not name_match:
            continue
        constraint_name = name_match.group(1)

        # Check if the claimed type is 'snare' within the INDEXICAL AUDIT section
        claimed_type_match = re.search(r'Constraint: ' + re.escape(constraint_name) + r'\s*\n\s*Claimed Type: (\w+)', chunk)
        if not claimed_type_match or claimed_type_match.group(1) != 'snare':
            continue # Not a claimed snare, so skip for this report

        # Found a claimed snare, now extract all its details
        snare_data = {
            'name': constraint_name,
            'claimed_type': claimed_type_match.group(1),
            'powerless_view': 'N/A',
            'institutional_view': 'N/A',
            'analytical_view': 'N/A',
            'structural_signature': 'N/A',
            'related_gap_alert': 'N/A',
            'omega_question': 'N/A',
            'severity': 'N/A',
            'resolution_strategy': ''
        }

        # Extract perspective classifications from the INDEXICAL AUDIT section
        perspectives_section = re.search(r'\[CONSTRAINT INVENTORY: INDEXICAL AUDIT\]\s*\n.*?Perspectives:(.*?)(?=\n\n|\n\[CROSS-DOMAIN ISOMORPHISM)', chunk, re.DOTALL)
        if perspectives_section:
            for line in perspectives_section.group(1).split('\n'):
                if 'Individual (Powerless):' in line:
                    snare_data['powerless_view'] = re.search(r': (\w+)', line).group(1)
                elif 'Institutional (Manager):' in line:
                    snare_data['institutional_view'] = re.search(r': (\w+)', line).group(1)
                elif 'Analytical:' in line: 
                    snare_data['analytical_view'] = re.search(r': (\w+)', line).group(1)

        # Extract Structural Signature Analysis
        signature_match = re.search(r'\[STRUCTURAL SIGNATURE ANALYSIS\]\s*\n.*?â†’ (.+)', chunk, re.DOTALL)
        if signature_match:
            snare_data['structural_signature'] = signature_match.group(1).strip()
        
        # Extract related gap/alert (if any)
        # Look for this in the PERSPECTIVAL GAP ANALYSIS section specifically
        gap_analysis_section = re.search(r'\[PERSPECTIVAL GAP ANALYSIS\](.*?)(?=\n\[OMEGA GENERATION|\n\[OMEGA TRIAGE)', chunk, re.DOTALL)
        if gap_analysis_section:
            gap_alert_match = re.search(r'(! (?:ALERT|GAP): .+)', gap_analysis_section.group(1))
            if gap_alert_match:
                snare_data['related_gap_alert'] = gap_alert_match.group(1).strip()

        # Extract Omega from OMEGA GENERATION section
        omega_section = re.search(r'\[OMEGA GENERATION FROM PERSPECTIVAL GAPS: ' + re.escape(constraint_name) + r'\](.*?)(?=\n\[OMEGA TRIAGE)', chunk, re.DOTALL)
        if omega_section:
            omega_match = re.search(r'Î©: (.+)', omega_section.group(1))
            if omega_match:
                snare_data['omega_question'] = omega_match.group(1).strip()

        # Extract Severity from the Triage section
        triage_section = re.search(r'\[OMEGA TRIAGE & PRIORITIZATION\](.*?)(?=\n\n|\n\[OMEGA RESOLUTION)', chunk, re.DOTALL)
        if triage_section:
            if '[critical]' in triage_section.group(1):
                snare_data['severity'] = 'critical'
            elif '[high]' in triage_section.group(1): # Prioritize critical over high
                snare_data['severity'] = 'high'
        
        # Extract Resolution Strategy
        res_match = re.search(r'RESOLUTION STRATEGY:\s*\n(.*?)(?:\n\s*â””â”€|\Z)', chunk, re.DOTALL)
        if res_match:
            strategy = res_match.group(1).strip()
            cleaned_strategy = "\n".join(line.strip().lstrip('â”‚').lstrip() for line in strategy.split('\n'))
            snare_data['resolution_strategy'] = cleaned_strategy

        # Add to list if we have the core info and it's a genuine snare
        if snare_data['claimed_type'] == 'snare':
            snares.append(snare_data)

    # Deduplicate based on constraint name and omega question, if multiple omegas were generated for the same constraint
    unique_snares = []
    seen = set()
    for n in snares:
        identifier = (n['name'], n['omega_question']) 
        if identifier not in seen:
            unique_snares.append(n)
            seen.add(identifier)

    return unique_snares

def generate_markdown_report(snares_data, output_path):
    """
    Generates a Markdown report from the list of Snare data.
    """
    # Sort by severity (critical first), then by name
    sorted_snares = sorted(snares_data, key=lambda x: (x['severity'] != 'critical', x['name']))

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# Snare Diagnostic Report\n\n")
        f.write(f"**Total Unique Snares Found:** {len(sorted_snares)}\n\n")
        f.write("---\n\n")

        for i, n in enumerate(sorted_snares, 1):
            f.write(f"### {i}. Snare: `{n['name']}`\n\n")
            f.write(f"*   **Claimed Type:** `{n['claimed_type']}`\n")
            f.write(f"*   **Severity:** `{n['severity']}`\n")
            
            f.write(f"*   **Perspectival Breakdown:**\n")
            if n['powerless_view'] != 'N/A':
                f.write(f"    *   **Individual (Powerless) View:** `{n['powerless_view']}`\n")
            if n['institutional_view'] != 'N/A':
                f.write(f"    *   **Institutional (Manager) View:** `{n['institutional_view']}`\n")
            if n['analytical_view'] != 'N/A': 
                f.write(f"    *   **Analytical View:** `{n['analytical_view']}`\n")
            
            f.write(f"*   **Structural Signature Analysis:** {n['structural_signature']}\n")
            
            if n['related_gap_alert'] != 'N/A':
                f.write(f"*   **Related Gap/Alert:** {n['related_gap_alert']}\n")
            
            f.write(f"*   **Generated Omega:** {n['omega_question']}\n")
            f.write(f"*   **Suggested Resolution Strategy:**\n")
            f.write(f"    ```\n{n['resolution_strategy']}\n    ```\n\n")
            f.write("---\n\n")

def main():
    """
    Main function to run the reporter.
    """
    script_dir = Path(__file__).parent
    log_file = script_dir / '../outputs/output.txt'
    report_file = script_dir / '../outputs/snare_report.md'
    
    print("Parsing log file to find Snares...")
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            log_content = f.read()
    except FileNotFoundError:
        print(f"Error: Log file not found at {log_file}", file=sys.stderr)
        sys.exit(1)
        
    snares_data = parse_log_content(log_content)
    
    if snares_data:
        print(f"Found {len(snares_data)} unique Snares.")
        print(f"Generating report at {report_file}...")
        generate_markdown_report(snares_data, report_file)
        print("Report generated successfully.")
    else:
        print("No Snares found in the log file.")

if __name__ == '__main__':
    main()
import os
import re
import sys

def lint_file(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()

    errors = []

    # 1. CORE STRUCTURE & v4.0 INTEGRATION HOOKS
    if not re.search(r':- module\(', content):
        errors.append("MISSING_MODULE: Prolog files must begin with :- module(id, []).")

    if "narrative_ontology:interval(" not in content:
        errors.append("MISSING_HOOK: Missing narrative_ontology:interval/3.")

    # Updated to check for v4.0 indexed classification
    if "constraint_classification(" not in content:
        errors.append("OUTDATED_HOOK: v4.0 requires constraint_indexing:constraint_classification/3.")

    # 2. PERSPECTIVAL MINIMUMS
    if "agent_power(individual_powerless)" not in content:
        errors.append("MISSING_PERSPECTIVE: Must include agent_power(individual_powerless).")

    if "agent_power(institutional)" not in content:
        errors.append("MISSING_PERSPECTIVE: Must include agent_power(institutional).")

    # 3. TYPE VARIANCE CHECK (Updated for v3.4 Categories)
    found_types = set(re.findall(r'constraint_classification\(.*?,[\s\n\r]*(mountain|rope|snare|tangled_rope|scaffold|piton)', content, re.DOTALL))
    if len(found_types) < 2:
        errors.append(f"INSUFFICIENT_VARIANCE: Found {list(found_types)}. Need at least 2 different types across indices.")

    # 4. DEPRECATED TERMINOLOGY
    if "noose" in content.lower():
        errors.append("DEPRECATED_TERM: 'noose' has been replaced by 'snare' in v3.4.")

    # 5. MANDATROPHY & OMEGA VALIDATION
    ext_match = re.search(r'base_extractiveness\(.*,\s*([\d\.]+)\)', content)
    if ext_match:
        val = float(ext_match.group(1))
        if val > 0.7:
            if "is_mandatrophy_resolved" not in content and "[RESOLVED MANDATROPHY]" not in content:
                errors.append(f"UNRESOLVED_MANDATROPHY: Extraction {val} requires resolution hook.")
        if val > 0.46 and "omega_variable(" not in content:
            errors.append("MISSING_OMEGA: High-extraction constraints (> 0.46) require omega_variable/5.")

    return errors
#!/usr/bin/env python3
"""
Sufficiency Tester - Report 2: Index Sufficiency Test

Determines if the 4 indices (agent_power, time_horizon, exit_options, spatial_scope)
are sufficient to explain all constraint type variance, or if new categories are needed.
"""

import json
from collections import defaultdict, Counter
from pathlib import Path

class SufficiencyTester:
    """Tests whether 4 indices sufficiently explain constraint variance"""

    def __init__(self, corpus_data_path):
        with open(corpus_data_path, 'r') as f:
            data = json.load(f)

        self.constraints = data['constraints']
        self.summary = data['summary']

    def analyze(self):
        """Run complete sufficiency analysis"""
        results = {}

        results['index_collisions'] = self.detect_index_collisions()
        results['collision_patterns'] = self.analyze_collision_patterns(results['index_collisions'])
        results['domain_sufficiency'] = self.analyze_domain_sufficiency()
        results['stability_anomalies'] = self.find_stability_anomalies()
        results['evidence_summary'] = self.summarize_evidence(results)

        return results

    def detect_index_collisions(self):
        """Find cases where same index config produces multiple types"""
        collisions = []

        for cid, constraint in self.constraints.items():
            classifications = constraint.get('classifications', [])

            if not classifications:
                continue

            # Group classifications by index configuration
            by_index = defaultdict(list)
            for classification in classifications:
                context = classification.get('context')
                ctype = classification.get('type')

                if context and ctype:
                    # Convert context to tuple for hashing
                    if isinstance(context, dict):
                        index_key = (
                            context.get('agent_power'),
                            context.get('time_horizon'),
                            context.get('exit_options'),
                            context.get('spatial_scope')
                        )
                    elif isinstance(context, tuple):
                        index_key = context
                    else:
                        # String representation - skip for collision detection
                        continue

                    by_index[index_key].append(ctype)

            # Find collisions (same index config -> multiple types)
            for index_config, types in by_index.items():
                unique_types = set(types)
                if len(unique_types) > 1:
                    collisions.append({
                        'constraint_id': cid,
                        'index_config': index_config,
                        'types_produced': list(unique_types),
                        'type_count': len(unique_types),
                        'domain': constraint.get('domain', 'unknown'),
                        'claimed_type': constraint.get('claimed_type'),
                        'extractiveness': constraint.get('metrics', {}).get('extractiveness'),
                        'suppression': constraint.get('metrics', {}).get('suppression')
                    })

        return collisions

    def analyze_collision_patterns(self, collisions):
        """Identify patterns in index collisions"""
        if not collisions:
            return {
                'total_collisions': 0,
                'constraints_affected': 0,
                'common_transitions': [],
                'domain_breakdown': []
            }

        # Count transitions (type A -> type B)
        transitions = Counter()
        constraints_affected = set()
        by_domain = defaultdict(list)

        for collision in collisions:
            constraints_affected.add(collision['constraint_id'])
            domain = collision['domain']
            by_domain[domain].append(collision)

            # Record all type pairs
            types = sorted(collision['types_produced'])
            if len(types) == 2:
                transition = f"{types[0]} â†” {types[1]}"
                transitions[transition] += 1

        # Domain breakdown
        domain_stats = []
        for domain in sorted(by_domain.keys()):
            domain_collisions = by_domain[domain]
            domain_stats.append({
                'domain': domain,
                'collision_count': len(domain_collisions),
                'constraints_affected': len(set(c['constraint_id'] for c in domain_collisions))
            })

        # Sort by frequency
        domain_stats.sort(key=lambda x: x['collision_count'], reverse=True)

        return {
            'total_collisions': len(collisions),
            'constraints_affected': len(constraints_affected),
            'common_transitions': transitions.most_common(10),
            'domain_breakdown': domain_stats
        }

    def analyze_domain_sufficiency(self):
        """Check if indices work better in some domains than others"""
        by_domain = defaultdict(lambda: {
            'total': 0,
            'with_collisions': 0,
            'avg_variance': 0.0,
            'variances': []
        })

        for cid, constraint in self.constraints.items():
            domain = constraint.get('domain', 'unknown')
            variance = constraint.get('analysis', {}).get('variance_ratio')

            by_domain[domain]['total'] += 1
            if variance is not None:
                by_domain[domain]['variances'].append(variance)

        # Calculate stats
        domain_stats = []
        for domain, stats in by_domain.items():
            if stats['variances']:
                avg_var = sum(stats['variances']) / len(stats['variances'])
                # High variance suggests indices ARE working (capturing real differences)
                # Low variance suggests indices might be MISSING something

                domain_stats.append({
                    'domain': domain,
                    'total_constraints': stats['total'],
                    'avg_variance': avg_var,
                    'sufficiency_score': avg_var  # Higher = indices capture more variance
                })

        # Sort by sufficiency score
        domain_stats.sort(key=lambda x: x['sufficiency_score'], reverse=True)

        return domain_stats

    def find_stability_anomalies(self):
        """Find constraints that are TOO stable (might need new categories)"""
        anomalies = []

        for cid, constraint in self.constraints.items():
            analysis = constraint.get('analysis', {})
            configs = analysis.get('index_configs', 0)
            types = analysis.get('types_produced', 0)
            variance = analysis.get('variance_ratio')

            # Anomaly: Many index configs but always same type
            # This suggests the constraint doesn't care about indices
            # Could indicate need for new category
            if configs >= 5 and types == 1:
                metrics = constraint.get('metrics', {})

                anomalies.append({
                    'constraint_id': cid,
                    'index_configs': configs,
                    'consistent_type': constraint.get('claimed_type'),
                    'domain': constraint.get('domain', 'unknown'),
                    'extractiveness': metrics.get('extractiveness'),
                    'suppression': metrics.get('suppression'),
                    'emerges_naturally': metrics.get('emerges_naturally'),
                    'requires_enforcement': metrics.get('requires_enforcement')
                })

        # Sort by number of configs tested
        anomalies.sort(key=lambda x: x['index_configs'], reverse=True)

        return anomalies

    def summarize_evidence(self, results):
        """Summarize evidence for/against index sufficiency"""
        collisions = results['index_collisions']
        patterns = results['collision_patterns']
        anomalies = results['stability_anomalies']

        total_constraints = len(self.constraints)
        constraints_with_collisions = patterns['constraints_affected']
        constraints_with_anomalies = len(anomalies)

        # Calculate percentages
        collision_pct = (constraints_with_collisions / total_constraints * 100) if total_constraints > 0 else 0
        anomaly_pct = (constraints_with_anomalies / total_constraints * 100) if total_constraints > 0 else 0

        # Evidence AGAINST sufficiency
        evidence_against = []
        if collision_pct > 10:
            evidence_against.append(f"{collision_pct:.1f}% of constraints have index collisions")
        if anomaly_pct > 20:
            evidence_against.append(f"{anomaly_pct:.1f}% of constraints are index-invariant")
        if collisions:
            common_transition = patterns['common_transitions'][0] if patterns['common_transitions'] else None
            if common_transition:
                evidence_against.append(f"Most common collision: {common_transition[0]} ({common_transition[1]} cases)")

        # Evidence FOR sufficiency
        evidence_for = []
        stable_pct = 100 - collision_pct
        if collision_pct < 5:
            evidence_for.append(f"{stable_pct:.1f}% of constraints have no index collisions")

        # Check domain coverage
        domain_stats = results['domain_sufficiency']
        if domain_stats:
            high_sufficiency_domains = [d for d in domain_stats if d['sufficiency_score'] > 0.7]
            if high_sufficiency_domains:
                evidence_for.append(f"{len(high_sufficiency_domains)} domains show high index sufficiency")

        return {
            'collision_rate': collision_pct,
            'anomaly_rate': anomaly_pct,
            'evidence_against_sufficiency': evidence_against,
            'evidence_for_sufficiency': evidence_for,
            'verdict': self._calculate_verdict(collision_pct, anomaly_pct)
        }

    def _calculate_verdict(self, collision_pct, anomaly_pct):
        """Determine overall verdict on index sufficiency"""
        if collision_pct > 20 or anomaly_pct > 30:
            return "INSUFFICIENT - Indices do not explain all variance. New categories recommended."
        elif collision_pct > 10 or anomaly_pct > 20:
            return "MIXED - Indices work for most cases but gaps exist. Consider hybrid approach."
        else:
            return "SUFFICIENT - Indices explain most variance. Current framework adequate."

    def generate_report(self, output_path):
        """Generate markdown report"""
        results = self.analyze()

        with open(output_path, 'w') as f:
            f.write("# Index Sufficiency Test\n\n")

            # Executive Summary
            f.write("## Executive Summary\n\n")
            summary = results['evidence_summary']
            f.write(f"**Verdict:** {summary['verdict']}\n\n")
            f.write(f"- **Collision Rate:** {summary['collision_rate']:.1f}%\n")
            f.write(f"- **Anomaly Rate:** {summary['anomaly_rate']:.1f}%\n\n")

            # Evidence Against Sufficiency
            if summary['evidence_against_sufficiency']:
                f.write("### Evidence Against Index Sufficiency\n\n")
                for evidence in summary['evidence_against_sufficiency']:
                    f.write(f"- {evidence}\n")
                f.write("\n")

            # Evidence For Sufficiency
            if summary['evidence_for_sufficiency']:
                f.write("### Evidence For Index Sufficiency\n\n")
                for evidence in summary['evidence_for_sufficiency']:
                    f.write(f"- {evidence}\n")
                f.write("\n")

            # Index Collisions
            f.write("## Index Collisions\n\n")
            collisions = results['index_collisions']
            patterns = results['collision_patterns']

            f.write(f"**Total collisions detected:** {patterns['total_collisions']}\n")
            f.write(f"**Constraints affected:** {patterns['constraints_affected']}\n\n")

            if patterns['common_transitions']:
                f.write("### Most Common Type Transitions\n\n")
                f.write("| Transition | Frequency |\n")
                f.write("|------------|----------|\n")
                for transition, count in patterns['common_transitions']:
                    f.write(f"| {transition} | {count} |\n")
                f.write("\n")

            # Domain Breakdown
            if patterns['domain_breakdown']:
                f.write("### Collisions by Domain\n\n")
                f.write("| Domain | Collisions | Constraints Affected |\n")
                f.write("|--------|-----------|---------------------|\n")
                for domain_stat in patterns['domain_breakdown']:
                    domain = domain_stat['domain'] or 'unknown'
                    f.write(f"| {domain:20s} | {domain_stat['collision_count']:10d} | {domain_stat['constraints_affected']:20d} |\n")
                f.write("\n")

            # Collision Examples
            if collisions:
                f.write("### Collision Examples\n\n")
                f.write("Cases where same index configuration produces multiple types:\n\n")
                f.write("| Constraint ID | Index Config | Types Produced | Domain |\n")
                f.write("|---------------|--------------|----------------|--------|\n")

                for collision in collisions[:15]:
                    types_str = ', '.join(collision['types_produced'])
                    index_str = str(collision['index_config'])[:30] + "..."
                    domain = collision['domain'] or 'unknown'
                    f.write(f"| {collision['constraint_id']:30s} | {index_str:30s} | {types_str:20s} | {domain:10s} |\n")

                f.write("\n")

            # Domain Sufficiency Analysis
            f.write("## Domain Sufficiency Analysis\n\n")
            f.write("How well do indices explain variance in each domain?\n\n")
            f.write("| Domain | Constraints | Avg Variance | Sufficiency |\n")
            f.write("|--------|-------------|--------------|-------------|\n")

            for domain_stat in results['domain_sufficiency']:
                score = domain_stat['sufficiency_score']
                rating = "High" if score > 0.7 else "Medium" if score > 0.4 else "Low"
                domain = domain_stat['domain'] or 'unknown'
                f.write(f"| {domain:20s} | {domain_stat['total_constraints']:11d} | {domain_stat['avg_variance']:12.2f} | {rating:11s} |\n")

            f.write("\n")
            f.write("**Note:** Higher variance = indices capture more differences (good)\n\n")

            # Stability Anomalies
            if results['stability_anomalies']:
                f.write("## Stability Anomalies\n\n")
                f.write("Constraints tested across many index configs but always produce same type.\n")
                f.write("These may indicate need for new categories beyond current 4 indices.\n\n")

                f.write("| Constraint ID | Configs | Type | Domain | Notes |\n")
                f.write("|---------------|---------|------|--------|-------|\n")

                for anomaly in results['stability_anomalies'][:20]:
                    notes = []
                    if anomaly.get('emerges_naturally'):
                        notes.append("natural")
                    if anomaly.get('requires_enforcement'):
                        notes.append("enforced")
                    notes_str = ', '.join(notes) if notes else '-'
                    consistent_type = anomaly['consistent_type'] or 'N/A'
                    domain = anomaly['domain'] or 'unknown'

                    f.write(f"| {anomaly['constraint_id']:30s} | {anomaly['index_configs']:7d} | {consistent_type:15s} | {domain:10s} | {notes_str} |\n")

                f.write("\n")

            # Recommendations
            f.write("## Recommendations\n\n")

            if summary['collision_rate'] > 15:
                f.write("1. **High collision rate detected.** Consider:\n")
                f.write("   - Adding 5th index dimension\n")
                f.write("   - Creating hybrid categories (Tangled Rope, Piton, etc.)\n")
                f.write("   - Refining existing index definitions\n\n")

            if summary['anomaly_rate'] > 25:
                f.write("2. **Many index-invariant constraints.** These may represent:\n")
                f.write("   - True natural laws (legitimately invariant)\n")
                f.write("   - Need for categorical rather than indexical classification\n")
                f.write("   - Candidates for new categories: Scaffold, Wings\n\n")

            if summary['collision_rate'] < 10 and summary['anomaly_rate'] < 20:
                f.write("1. **Current 4 indices appear sufficient** for most constraints.\n")
                f.write("2. Consider adding new categories only for edge cases.\n")
                f.write("3. Focus on refining metric thresholds rather than structural changes.\n\n")

        print(f"Report generated: {output_path}")

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Generate index sufficiency test (Report 2)'
    )
    parser.add_argument(
        '--corpus-data',
        default='../outputs/corpus_data.json',
        help='Path to corpus_data.json'
    )
    parser.add_argument(
        '--output',
        default='../outputs/index_sufficiency.md',
        help='Output markdown file'
    )

    args = parser.parse_args()

    tester = SufficiencyTester(args.corpus_data)
    tester.generate_report(args.output)

if __name__ == '__main__':
    main()
import re
import sys
from pathlib import Path

def parse_log_content(content):
    """
    Parses the log content to find and extract details for any constraint
    that is classified as a Tangled Rope from any perspective.
    """
    tangled_ropes = []
    
    # Split the entire log into chunks, each starting with a new scenario load.
    scenario_chunks = re.split(r'(?=\[SCENARIO MANAGER\] Clearing Knowledge Base...)', content)

    for chunk in scenario_chunks:
        if not chunk.strip():
            continue

        # Get the name of the constraint being tested in this chunk
        name_match = re.search(r'Loading: \.\./prolog/testsets/(.+?)\.pl', chunk)
        if not name_match:
            continue
        constraint_name = name_match.group(1)

        # The trigger: Find 'tangled_rope' in any perspective classification
        perspectives_section_match = re.search(r'\[CONSTRAINT INVENTORY: INDEXICAL AUDIT\]\s*\n(.*?)(?=\n\n\[CROSS-DOMAIN ISOMORPHISM|\Z)', chunk, re.DOTALL)
        if not perspectives_section_match:
            continue

        perspectives_section = perspectives_section_match.group(1)
        if 'tangled_rope' not in perspectives_section:
            continue # This scenario does not contain a tangled rope classification.

        tr_data = {
            'name': constraint_name,
            'claimed_type': 'N/A',
            'powerless_view': 'N/A',
            'institutional_view': 'N/A',
            'analytical_view': 'N/A',
            'structural_signature': 'N/A',
            'related_gap_alert': 'N/A',
            'omega_question': 'N/A',
            'severity': 'N/A',
            'resolution_strategy': ''
        }

        # Extract original claimed type
        claimed_type_match = re.search(r'^\s*Claimed Type: (\w+)', perspectives_section, re.MULTILINE)
        if claimed_type_match:
            tr_data['claimed_type'] = claimed_type_match.group(1)

        # FINAL FIX: More robust perspective parsing
        for line in perspectives_section.split('\n'):
            # Match the pattern like "- [context...]: classification (Match...)"
            match = re.search(r'-\s\[context\(.*?\)\]:\s*(\w+)', line)
            if not match:
                continue
            
            classification = match.group(1)
            if 'individual_powerless' in line:
                tr_data['powerless_view'] = classification
            elif 'institutional' in line:
                tr_data['institutional_view'] = classification
            elif 'analytical' in line:
                tr_data['analytical_view'] = classification

        # Extract Structural Signature Analysis (non-greedy)
        signature_match = re.search(r'â†’\s*(.+)', chunk)
        if signature_match:
            tr_data['structural_signature'] = signature_match.group(1).strip()
        
        # Extract related gap/alert (if any)
        gap_alert_match = re.search(r'(!\s(?:ALERT|GAP):\s*.+)', chunk)
        if gap_alert_match:
            tr_data['related_gap_alert'] = gap_alert_match.group(1).strip()

        # Extract Omega Question specifically
        omega_question_match = re.search(r'Question:\s*(.+)', chunk)
        if omega_question_match:
            tr_data['omega_question'] = omega_question_match.group(1).strip()

        # Extract Severity from the Triage section
        triage_section_match = re.search(r'\[OMEGA TRIAGE & PRIORITIZATION\](.*?)(?=\n\n|\n\[OMEGA RESOLUTION)', chunk, re.DOTALL)
        if triage_section_match:
            if '[critical]' in triage_section_match.group(1):
                tr_data['severity'] = 'critical'
            elif '[high]' in triage_section_match.group(1):
                tr_data['severity'] = 'high'
        
        # Extract Resolution Strategy
        res_match = re.search(r'RESOLUTION STRATEGY:\s*\n(.*?)(?:\n\s*â””â”€|\n\n### START LLM REFINEMENT MANIFEST|\Z)', chunk, re.DOTALL)
        if res_match:
            strategy = res_match.group(1).strip()
            cleaned_strategy = "\n".join(line.strip().lstrip('â”‚').lstrip() for line in strategy.split('\n'))
            tr_data['resolution_strategy'] = cleaned_strategy.strip()

        tangled_ropes.append(tr_data)

    # Deduplicate
    unique_trs = []
    seen = set()
    for tr in tangled_ropes:
        identifier = (tr['name'], tr['omega_question']) 
        if identifier not in seen:
            unique_trs.append(tr)
            seen.add(identifier)

    return unique_trs

def generate_markdown_report(tr_data, output_path):
    """
    Generates a Markdown report from the list of Tangled Rope data.
    """
    sorted_trs = sorted(tr_data, key=lambda x: (x['severity'] != 'critical', x['name']))

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# Tangled Rope Diagnostic Report\n\n")
        f.write(f"**Total Unique Tangled Ropes Found:** {len(sorted_trs)}\n\n")
        f.write("---\n\n")

        for i, tr in enumerate(sorted_trs, 1):
            f.write(f"### {i}. Tangled Rope: `{tr['name']}`\n\n")
            f.write(f"*   **Claimed Type:** `{tr['claimed_type']}`\n")
            f.write(f"*   **Severity:** `{tr['severity']}`\n")
            
            f.write(f"*   **Perspectival Breakdown:**\n")
            f.write(f"    *   Individual (Powerless) View: `{tr['powerless_view']}`\n")
            f.write(f"    *   Institutional (Manager) View: `{tr['institutional_view']}`\n")
            f.write(f"    *   Analytical View: `{tr['analytical_view']}`\n")
            
            f.write(f"*   **Structural Signature Analysis:** {tr['structural_signature']}\n")
            f.write(f"*   **Related Gap/Alert:** {tr['related_gap_alert']}\n")
            f.write(f"*   **Generated Omega:** {tr['omega_question']}\n")
            f.write(f"*   **Suggested Resolution Strategy:**\n")
            f.write(f"    ```\n{tr['resolution_strategy']}\n    ```\n\n")
            f.write("---\n\n")

def main():
    """
    Main function to run the reporter.
    """
    script_dir = Path(__file__).parent
    log_file = script_dir / '../outputs/output.txt'
    report_file = script_dir / '../outputs/tangled_rope_report.md'
    
    print("Parsing log file to find Tangled Ropes (based on analyzed perspectives)...")
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            log_content = f.read()
    except FileNotFoundError:
        print(f"Error: Log file not found at {log_file}", file=sys.stderr)
        sys.exit(1)
        
    tr_data = parse_log_content(log_content)
    
    if tr_data:
        print(f"Found {len(tr_data)} constraints classified as Tangled Ropes.")
        print(f"Generating report at {report_file}...")
        generate_markdown_report(tr_data, report_file)
        print("Report generated successfully.")
    else:
        print("No constraints classified as Tangled Ropes found in the log file.")

if __name__ == '__main__':
    main()
import re
import sys
from pathlib import Path

def parse_log_content(content):
    """
    Parses the log content to find and extract True Mountain details.
    A True Mountain is defined as a constraint that is both claimed as a mountain
    and is classified as a mountain from all perspectives (no mismatches).
    """
    true_mountains = []
    
    scenario_chunks = re.split(r'(?=\[SCENARIO MANAGER\] Clearing Knowledge Base...)', content)

    for chunk in scenario_chunks:
        if not chunk.strip():
            continue

        name_match = re.search(r'Loading: \.\./prolog/testsets/(.+?)\.pl', chunk)
        if not name_match:
            continue
        constraint_name = name_match.group(1)

        # Look for the main audit section
        audit_section_match = re.search(r'\[CONSTRAINT INVENTORY: INDEXICAL AUDIT\]\s*\n(.*?)(?=\n\n\[CROSS-DOMAIN ISOMORPHISM|\Z)', chunk, re.DOTALL)
        if not audit_section_match:
            continue
        
        audit_section = audit_section_match.group(1)

        # 1. Claimed Type must be 'mountain'
        claimed_type_match = re.search(r'^\s*Claimed Type: (mountain)', audit_section, re.MULTILINE)
        if not claimed_type_match:
            continue

        # 2. There must be at least one perspective, and NO mismatches
        if '(Mismatch)' in audit_section or 'Perspectives:' not in audit_section:
            continue
            
        # If we passed all checks, it's a True Mountain. Extract details.
        tm_data = {
            'name': constraint_name,
            'claimed_type': 'mountain',
            'structural_signature': 'N/A'
        }

        # Extract Structural Signature Analysis
        signature_match = re.search(r'â†’\s*(.+)', chunk)
        if signature_match:
            tm_data['structural_signature'] = signature_match.group(1).strip()
        
        true_mountains.append(tm_data)

    # Deduplicate (shouldn't be necessary with this logic, but good practice)
    unique_tms = []
    seen = set()
    for tm in true_mountains:
        identifier = tm['name']
        if identifier not in seen:
            unique_tms.append(tm)
            seen.add(identifier)

    return unique_tms

def generate_markdown_report(tm_data, output_path):
    """
    Generates a Markdown report from the list of True Mountain data.
    """
    sorted_tms = sorted(tm_data, key=lambda x: x['name'])

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# True Mountain Validation Report\n\n")
        f.write(f"**Total Validated:** {len(sorted_tms)}\n\n")
        f.write("This report lists all constraints that are consistently classified as 'mountain' across all tested perspectives, confirming their immutability within the model.\n\n")
        f.write("---\n\n")

        for i, tm in enumerate(sorted_tms, 1):
            f.write(f"### {i}. True Mountain: `{tm['name']}`\n\n")
            f.write(f"*   **Claimed Type:** `{tm['claimed_type']}`\n")
            f.write(f"*   **Structural Signature Analysis:** {tm['structural_signature']}\n")
            f.write(f"*   **Perspectival Agreement:** Confirmed. All tested perspectives agree on the 'mountain' classification.\n\n")
            f.write("---\n\n")

def main():
    """
    Main function to run the reporter.
    """
    script_dir = Path(__file__).parent
    log_file = script_dir / '../outputs/output.txt'
    report_file = script_dir / '../outputs/true_mountain_report.md'
    
    print("Parsing log file to find True Mountains...")
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            log_content = f.read()
    except FileNotFoundError:
        print(f"Error: Log file not found at {log_file}", file=sys.stderr)
        sys.exit(1)
        
    tm_data = parse_log_content(log_content)
    
    if tm_data:
        print(f"Found {len(tm_data)} validated True Mountains.")
        print(f"Generating report at {report_file}...")
        generate_markdown_report(tm_data, report_file)
        print("Report generated successfully.")
    else:
        print("No validated True Mountains found in the log file.")

if __name__ == '__main__':
    main()
#!/usr/bin/env python3
"""
Enhanced Constraint Validator
Provides syntax checking and schema verification for Deferential Realism stories.
"""

import re
import sys
import subprocess
from pathlib import Path

class ConstraintValidator:
    def __init__(self, filepath):
        self.filepath = Path(filepath)
        try:
            self.content = self.filepath.read_text()
        except Exception as e:
            print(f"Error reading file: {e}")
            sys.exit(1)
        self.issues = []
        self.warnings = []

    def validate(self):
        """Run all validation checks."""
        self.check_syntax()
        self.check_module_declaration()
        self.check_multifile_declarations()
        self.check_schema_types()
        self.check_interval_declaration()

        return len(self.issues) == 0

    def check_syntax(self):
        """Verify Prolog syntax using the SWI-Prolog interpreter."""
        try:
            # Use swipl -s (load file) and -g halt (exit) to check for syntax errors
            result = subprocess.run(
                ['swipl', '-s', str(self.filepath), '-g', 'halt'],
                capture_output=True,
                text=True
            )
            if result.returncode != 0:
                # Capture the specific error from stderr
                error_msg = result.stderr.strip()
                self.issues.append(f"Prolog Syntax Error:\n{error_msg}")
        except FileNotFoundError:
            self.warnings.append("swipl not found in PATH; skipping deep syntax check")

    def check_module_declaration(self):
        """Verify the file has a proper module declaration."""
        if not re.search(r':- module\([^)]+,\s*\[\s*\]\)', self.content):
            self.issues.append("Missing or malformed module declaration: :- module(name, []).")

    def check_multifile_declarations(self):
        """Check for required multifile hooks for domain priors."""
        required_hooks = [
            'domain_priors:base_extractiveness/2',
            'domain_priors:suppression_score/2',
            'domain_priors:requires_active_enforcement/1'
        ]

        for hook in required_hooks:
            if hook not in self.content:
                self.warnings.append(f"Missing hook: {hook}")

    def check_schema_types(self):
        """Ensure the constraint_claim uses a recognized ontological category."""
        # Updated to include natural_law, election_cycle, and physical_law
        valid_claims = [
            'mountain', 'snare', 'rope', 'piton',
            'natural_law', 'physical_law', 'election_cycle',
            'statutory_limit', 'market_constraint'
        ]

        claim_match = re.search(r'constraint_claim\([^,]+,\s*([^)]+)\)', self.content)
        if claim_match:
            claim_type = claim_match.group(1).strip()
            if claim_type not in valid_claims:
                self.issues.append(f"Invalid constraint_claim type: {claim_type}")

    def check_interval_declaration(self):
        """Verify the narrative interval is defined."""
        if 'narrative_ontology:interval' not in self.content:
            self.issues.append("Missing narrative interval declaration")

    def report(self):
        """Print validation report."""
        print(f"\n{'='*60}")
        print(f"Validation Report: {self.filepath.name}")
        print(f"{'='*60}\n")

        if not self.issues and not self.warnings:
            print("âœ“ All checks passed!")
            return True

        if self.issues:
            print("ISSUES (must fix):")
            for i, issue in enumerate(self.issues, 1):
                print(f"  {i}. âœ— {issue}")
            print()

        if self.warnings:
            print("WARNINGS (review recommended):")
            for i, warning in enumerate(self.warnings, 1):
                print(f"  {i}. âš  {warning}")
            print()

        return len(self.issues) == 0

def main():
    if len(sys.argv) < 2:
        print("Usage: python3 validator.py <file.pl>")
        sys.exit(1)

    validator = ConstraintValidator(sys.argv[1])
    validator.validate()
    validator.report()

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
Variance Analyzer - Report 1: Index Variance Analysis

Determines if constraints naturally cluster or if indices explain all variance.
"""

import json
from collections import defaultdict, Counter
from pathlib import Path

class VarianceAnalyzer:
    """Analyzes variance across index configurations"""

    def __init__(self, corpus_data_path):
        with open(corpus_data_path, 'r') as f:
            data = json.load(f)

        self.constraints = data['constraints']
        self.summary = data['summary']

    def analyze(self):
        """Run complete variance analysis"""
        results = {}

        results['summary'] = self.calculate_summary_statistics()
        results['variance_distribution'] = self.analyze_variance_distribution()
        results['domain_breakdown'] = self.analyze_by_domain()
        results['high_variance_examples'] = self.find_high_variance_examples()
        results['suspicious_stability'] = self.find_suspicious_stability()

        return results

    def calculate_summary_statistics(self):
        """Calculate overall corpus statistics"""
        total = len(self.constraints)

        with_multiple_configs = sum(
            1 for c in self.constraints.values()
            if c['analysis']['index_configs'] and c['analysis']['index_configs'] > 1
        )

        # Count by variance ratio
        high_variance = sum(
            1 for c in self.constraints.values()
            if c['analysis']['variance_ratio'] and c['analysis']['variance_ratio'] > 0.5
        )

        stable = sum(
            1 for c in self.constraints.values()
            if c['analysis']['variance_ratio'] == 1.0
        )

        return {
            'total_constraints': total,
            'with_multiple_configs': with_multiple_configs,
            'with_multiple_configs_pct': (with_multiple_configs / total * 100) if total > 0 else 0,
            'high_variance_count': high_variance,
            'high_variance_pct': (high_variance / total * 100) if total > 0 else 0,
            'stable_count': stable,
            'stable_pct': (stable / total * 100) if total > 0 else 0
        }

    def analyze_variance_distribution(self):
        """Break down variance ratios into buckets"""
        buckets = {
            '1.0 (stable)': [],
            '0.7-0.9': [],
            '0.5-0.6': [],
            '0.3-0.4': [],
            '<0.3': [],
            'null': []
        }

        for cid, constraint in self.constraints.items():
            ratio = constraint['analysis']['variance_ratio']

            if ratio is None:
                buckets['null'].append(cid)
            elif ratio == 1.0:
                buckets['1.0 (stable)'].append(cid)
            elif 0.7 <= ratio < 1.0:
                buckets['0.7-0.9'].append(cid)
            elif 0.5 <= ratio < 0.7:
                buckets['0.5-0.6'].append(cid)
            elif 0.3 <= ratio < 0.5:
                buckets['0.3-0.4'].append(cid)
            else:
                buckets['<0.3'].append(cid)

        # Convert to summary
        total = len(self.constraints)
        distribution = []

        for bucket_name in ['1.0 (stable)', '0.7-0.9', '0.5-0.6', '0.3-0.4', '<0.3', 'null']:
            constraints = buckets[bucket_name]
            count = len(constraints)
            pct = (count / total * 100) if total > 0 else 0

            # Get examples
            examples = constraints[:3] if len(constraints) > 0 else []

            distribution.append({
                'range': bucket_name,
                'count': count,
                'percentage': pct,
                'examples': examples
            })

        return distribution

    def analyze_by_domain(self):
        """Variance analysis broken down by domain"""
        by_domain = defaultdict(list)

        for cid, constraint in self.constraints.items():
            domain = constraint.get('domain', 'unknown')
            ratio = constraint['analysis']['variance_ratio']

            if ratio is not None:
                by_domain[domain].append(ratio)

        domain_stats = []
        # Filter out None domains and sort
        valid_domains = [d for d in by_domain.keys() if d is not None]
        for domain in sorted(valid_domains):
            ratios = by_domain[domain]
            n = len(ratios)

            if n == 0:
                continue

            avg_variance = sum(ratios) / n
            high_variance_count = sum(1 for r in ratios if r > 0.5)
            high_variance_pct = (high_variance_count / n * 100)

            domain_stats.append({
                'domain': domain,
                'n': n,
                'avg_variance': avg_variance,
                'high_variance_pct': high_variance_pct
            })

        # Sort by avg variance descending
        domain_stats.sort(key=lambda x: x['avg_variance'], reverse=True)

        return domain_stats

    def find_high_variance_examples(self, top_n=10):
        """Find most volatile constraints"""
        candidates = []

        for cid, constraint in self.constraints.items():
            ratio = constraint['analysis']['variance_ratio']
            if ratio and ratio > 0.5:
                candidates.append({
                    'constraint_id': cid,
                    'variance_ratio': ratio,
                    'index_configs': constraint['analysis']['index_configs'],
                    'types_produced': constraint['analysis']['types_produced'],
                    'domain': constraint.get('domain', 'unknown'),
                    'claimed_type': constraint.get('claimed_type'),
                    'classifications': constraint.get('classifications', [])
                })

        # Sort by variance ratio descending
        candidates.sort(key=lambda x: x['variance_ratio'], reverse=True)

        return candidates[:top_n]

    def find_suspicious_stability(self, threshold=5):
        """Find constraints that SHOULD vary but don't"""
        # Constraints with many index configs but low variance
        suspicious = []

        for cid, constraint in self.constraints.items():
            configs = constraint['analysis']['index_configs']
            ratio = constraint['analysis']['variance_ratio']

            if configs and configs >= threshold and ratio and ratio < 0.3:
                suspicious.append({
                    'constraint_id': cid,
                    'index_configs': configs,
                    'types_produced': constraint['analysis']['types_produced'],
                    'variance_ratio': ratio,
                    'domain': constraint.get('domain', 'unknown'),
                    'claimed_type': constraint.get('claimed_type')
                })

        # Sort by number of configs
        suspicious.sort(key=lambda x: x['index_configs'], reverse=True)

        return suspicious

    def generate_report(self, output_path):
        """Generate markdown report"""
        results = self.analyze()

        with open(output_path, 'w') as f:
            f.write("# Index Variance Analysis\n\n")

            # Summary Statistics
            f.write("## Summary Statistics\n\n")
            summary = results['summary']
            f.write(f"- **Total constraints analyzed:** {summary['total_constraints']}\n")
            f.write(f"- **Constraints with multiple index configs:** {summary['with_multiple_configs']} ({summary['with_multiple_configs_pct']:.1f}%)\n")
            f.write(f"- **High variance (>0.5):** {summary['high_variance_count']} ({summary['high_variance_pct']:.1f}%)\n")
            f.write(f"- **Stable (ratio=1.0):** {summary['stable_count']} ({summary['stable_pct']:.1f}%)\n\n")

            # Variance Distribution
            f.write("## Variance Distribution\n\n")
            f.write("| Ratio Range | Count | % of Corpus | Examples |\n")
            f.write("|-------------|-------|-------------|----------|\n")

            for bucket in results['variance_distribution']:
                examples_str = ', '.join(bucket['examples']) if bucket['examples'] else '-'
                if len(examples_str) > 50:
                    examples_str = examples_str[:47] + '...'

                f.write(f"| {bucket['range']:15s} | {bucket['count']:5d} | {bucket['percentage']:6.1f}% | {examples_str} |\n")

            f.write("\n")

            # Domain Breakdown
            f.write("## Domain Breakdown\n\n")
            f.write("| Domain | N | Avg Variance | High Variance % |\n")
            f.write("|--------|---|--------------|----------------|\n")

            for domain_stat in results['domain_breakdown']:
                f.write(f"| {domain_stat['domain']:20s} | {domain_stat['n']:3d} | {domain_stat['avg_variance']:12.2f} | {domain_stat['high_variance_pct']:14.1f}% |\n")

            f.write("\n")

            # Key Findings
            f.write("## Key Findings\n\n")

            # Automatically generated insights
            if results['domain_breakdown']:
                highest_variance_domain = results['domain_breakdown'][0]
                lowest_variance_domain = results['domain_breakdown'][-1]

                f.write(f"1. **Domain variance spread:** {highest_variance_domain['domain']} shows highest variance ({highest_variance_domain['avg_variance']:.2f}), while {lowest_variance_domain['domain']} shows lowest ({lowest_variance_domain['avg_variance']:.2f})\n\n")

            if summary['stable_pct'] > 50:
                f.write(f"2. **High stability:** {summary['stable_pct']:.1f}% of constraints are completely stable across index configs\n\n")
            elif summary['high_variance_pct'] > 30:
                f.write(f"2. **High volatility:** {summary['high_variance_pct']:.1f}% of constraints show high variance (>0.5)\n\n")

            if results['high_variance_examples']:
                f.write(f"3. **Perspective-dependent constraints:** {len(results['high_variance_examples'])} constraints show strong perspective-dependence\n\n")

            # High Variance Examples
            f.write("## High Variance Examples\n\n")
            f.write("Constraints that change type frequently based on index configuration:\n\n")
            f.write("| Constraint ID | Variance | Configs | Types | Domain | Claimed Type |\n")
            f.write("|---------------|----------|---------|-------|--------|-------------|\n")

            for example in results['high_variance_examples']:
                claimed = example.get('claimed_type') or 'N/A'
                domain = example['domain'] or 'unknown'
                f.write(f"| {example['constraint_id']:30s} | {example['variance_ratio']:.2f} | {example['index_configs']:7d} | {example['types_produced']:5d} | {domain:10s} | {claimed:12s} |\n")

            f.write("\n")

            # Detailed examples
            if results['high_variance_examples']:
                f.write("### Detailed Examples\n\n")
                for i, example in enumerate(results['high_variance_examples'][:3], 1):
                    f.write(f"**{i}. {example['constraint_id']}**\n")
                    f.write(f"- Domain: {example['domain']}\n")
                    f.write(f"- Variance: {example['variance_ratio']:.2f}\n")
                    f.write(f"- Produces {example['types_produced']} different types across {example['index_configs']} index configurations\n")

                    if example['classifications']:
                        type_counts = Counter(c['type'] for c in example['classifications'])
                        f.write(f"- Type distribution: {dict(type_counts)}\n")

                    f.write("\n")

            # Suspicious Stability
            if results['suspicious_stability']:
                f.write("## Suspicious Stability\n\n")
                f.write("Constraints with many index configs but low variance (possible modeling issues):\n\n")
                f.write("| Constraint ID | Configs | Types | Variance | Domain |\n")
                f.write("|---------------|---------|-------|----------|--------|\n")

                for item in results['suspicious_stability'][:10]:
                    f.write(f"| {item['constraint_id']:30s} | {item['index_configs']:7d} | {item['types_produced']:5d} | {item['variance_ratio']:8.2f} | {item['domain']:15s} |\n")

                f.write("\n")
                f.write("**Note:** These constraints have many perspective configurations but produce the same type. This might indicate:\n")
                f.write("- The constraint is genuinely invariant (e.g., physical laws)\n")
                f.write("- Index dimensions are not affecting classification\n")
                f.write("- Potential data quality issue\n\n")

            # Data Completeness
            f.write("## Data Completeness\n\n")
            f.write("| Field | % Complete | Impact |\n")
            f.write("|-------|-----------|--------|\n")

            total = summary['total_constraints']

            # Classifications
            with_class = sum(1 for c in self.constraints.values() if c.get('classifications'))
            f.write(f"| classifications | {(with_class/total*100):.1f}% | Core data for variance analysis |\n")

            # Variance ratio
            with_variance = sum(1 for c in self.constraints.values() if c['analysis']['variance_ratio'] is not None)
            f.write(f"| variance_ratio | {(with_variance/total*100):.1f}% | Calculated from classifications |\n")

            # Domain
            with_domain = sum(1 for c in self.constraints.values() if c.get('domain'))
            f.write(f"| domain | {(with_domain/total*100):.1f}% | Affects domain breakdown analysis |\n")

        print(f"Report generated: {output_path}")

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Generate variance analysis report (Report 1)'
    )
    parser.add_argument(
        '--corpus-data',
        default='../outputs/corpus_data.json',
        help='Path to corpus_data.json'
    )
    parser.add_argument(
        '--output',
        default='../outputs/variance_analysis.md',
        help='Output markdown file'
    )

    args = parser.parse_args()

    analyzer = VarianceAnalyzer(args.corpus_data)
    analyzer.generate_report(args.output)

if __name__ == '__main__':
    main()
